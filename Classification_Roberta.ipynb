{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/seilsepour/SSTSA/blob/main/Classification_Roberta.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-AUwBd8NDPGg"
      },
      "source": [
        "https://towardsdatascience.com/fine-tuning-bert-and-roberta-for-high-accuracy-text-classification-in-pytorch-c9e63cf64646"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lmAhOBhaCkfS"
      },
      "outputs": [],
      "source": [
        "data_path = '/MYDRIVE/My Drive/Colab Notebooks/SSTSA/data'\n",
        "output_path = '/MYDRIVE/My Drive/Colab Notebooks/SSTSA/data'\n",
        "dataSource = 'resultAll-MR-01-02-22.xlsx'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uDAg5SSdq5fv"
      },
      "outputs": [],
      "source": [
        "MAX_SEQ_LEN = 512\n",
        "BATCH_SIZE = 8"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7pJCQH7lEbUD"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/MYDRIVE', force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dVZeJujaF5V7"
      },
      "outputs": [],
      "source": [
        "!pip install -U torchtext==0.10.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sd931ByfGEpZ"
      },
      "outputs": [],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n3O6XKaxDWUL"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "\n",
        "import torch\n",
        "from torchtext.legacy.data import Field, TabularDataset, BucketIterator, Iterator\n",
        "#from torchtext.data import Field, TabularDataset, BucketIterator, Iterator\n",
        "\n",
        "from transformers import RobertaTokenizer, RobertaModel, AdamW, get_linear_schedule_with_warmup\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "import logging\n",
        "logging.getLogger(\"transformers.tokenization_utils_base\").setLevel(logging.ERROR)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9gVvtmbnAoTG"
      },
      "outputs": [],
      "source": [
        "df1 = pd.read_excel(f\"{data_path}/{dataSource}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0njsLOiHDaCK"
      },
      "outputs": [],
      "source": [
        "df1.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZFxMF4KuDko4"
      },
      "outputs": [],
      "source": [
        "df = df1[['id','MIX','ActualText']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_-SZJTX3EPg5"
      },
      "outputs": [],
      "source": [
        "df.columns = [\"id\",\"label\",\"text\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zWd03tLTEVEK"
      },
      "outputs": [],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KWUWv50euaKm"
      },
      "outputs": [],
      "source": [
        "# Load CSV file with dataset. Perform basic transformations.\n",
        "#df = pd.read_csv(f\"{data_path}/Pang-Ver2-00-10-24.csv\")\n",
        "#df = df.drop(['Unnamed: 0'], axis=1)\n",
        "#df = df.read_excel(dataSource)\n",
        "#df.columns = [\"id\",\"label\",\"text\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9d4E43G7uwln"
      },
      "outputs": [],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bu-4kQHoGv7v"
      },
      "outputs": [],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VExm6LXTDZ5E"
      },
      "outputs": [],
      "source": [
        "# Plot histogram with the length. Truncate max length to 5000 tokens.\n",
        "plt.style.use(\"ggplot\")\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "df['length'] = df['text'].apply(lambda x: len(x.split()))\n",
        "sns.distplot(df[df['length'] < 5000]['length'])\n",
        "plt.title('Frequence of documents of a given length', fontsize=14)\n",
        "plt.xlabel('length', fontsize=14)\n",
        "None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tA8y7eKRDc0u"
      },
      "outputs": [],
      "source": [
        "# Save preprocessed data, cropped to max length of the model.\n",
        "df['text'] = df['text'].apply(lambda x: \" \".join(x.split()[:512]))\n",
        "df.to_csv(f\"{data_path}/prep_panglee.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e51e7Po6DfzZ"
      },
      "outputs": [],
      "source": [
        "# Set random seed and set device to GPU.\n",
        "torch.manual_seed(17)\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device('cuda:0')\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "else:\n",
        "    device = torch.device('cpu')\n",
        "\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "azpkrSKLDkgj"
      },
      "outputs": [],
      "source": [
        "# Initialize tokenizer.\n",
        "tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qNb_9WjkDlGb"
      },
      "outputs": [],
      "source": [
        "# Set tokenizer hyperparameters.\n",
        "\n",
        "PAD_INDEX = tokenizer.convert_tokens_to_ids(tokenizer.pad_token)\n",
        "UNK_INDEX = tokenizer.convert_tokens_to_ids(tokenizer.unk_token)\n",
        "\n",
        "\n",
        "# Define columns to read.\n",
        "label_field = Field(sequential=False, use_vocab=False, batch_first=True)\n",
        "text_field = Field(use_vocab=False,\n",
        "                   tokenize=tokenizer.encode,\n",
        "                   include_lengths=False,\n",
        "                   batch_first=True,\n",
        "                   fix_length=MAX_SEQ_LEN,\n",
        "                   pad_token=PAD_INDEX,\n",
        "                   unk_token=UNK_INDEX)\n",
        "\n",
        "fields = {'text' : ('text', text_field), 'label' : ('label', label_field)}\n",
        "\n",
        "\n",
        "# Read preprocessed CSV into TabularDataset and split it into train, test and valid.\n",
        "train_data, valid_data, test_data = TabularDataset(path=f\"{data_path}/prep_panglee.csv\",\n",
        "                                                   format='CSV',\n",
        "                                                   fields=fields,\n",
        "                                                   skip_header=False).split(split_ratio=[0.80, 0.1, 0.1],\n",
        "                                                                            stratified=True,\n",
        "                                                                            strata_field='label')\n",
        "\n",
        "# Create train and validation iterators.\n",
        "train_iter, valid_iter = BucketIterator.splits((train_data, valid_data),\n",
        "                                               batch_size=BATCH_SIZE,\n",
        "                                               device=device,\n",
        "                                               shuffle=True,\n",
        "                                               sort_key=lambda x: len(x.text),\n",
        "                                               sort=True,\n",
        "                                               sort_within_batch=False)\n",
        "\n",
        "# Test iterator, no shuffling or sorting required.\n",
        "test_iter = Iterator(test_data, batch_size=BATCH_SIZE, device=device, train=False, shuffle=False, sort=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wq1LILiMDsOQ"
      },
      "outputs": [],
      "source": [
        "# Functions for saving and loading model parameters and metrics.\n",
        "def save_checkpoint(path, model, valid_loss):\n",
        "    torch.save({'model_state_dict': model.state_dict(),\n",
        "                  'valid_loss': valid_loss}, path)\n",
        "\n",
        "\n",
        "def load_checkpoint(path, model):\n",
        "    state_dict = torch.load(path, map_location=device)\n",
        "    model.load_state_dict(state_dict['model_state_dict'])\n",
        "\n",
        "    return state_dict['valid_loss']\n",
        "\n",
        "\n",
        "def save_metrics(path, train_loss_list, valid_loss_list, global_steps_list):\n",
        "    state_dict = {'train_loss_list': train_loss_list,\n",
        "                  'valid_loss_list': valid_loss_list,\n",
        "                  'global_steps_list': global_steps_list}\n",
        "\n",
        "    torch.save(state_dict, path)\n",
        "\n",
        "\n",
        "def load_metrics(path):\n",
        "    state_dict = torch.load(path, map_location=device)\n",
        "    return state_dict['train_loss_list'], state_dict['valid_loss_list'], state_dict['global_steps_list']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aKrmbzhyDyXk"
      },
      "outputs": [],
      "source": [
        "# Model with extra layers on top of RoBERTa\n",
        "class ROBERTAClassifier(torch.nn.Module):\n",
        "    def __init__(self, dropout_rate=0.3):\n",
        "        super(ROBERTAClassifier, self).__init__()\n",
        "\n",
        "        self.roberta = RobertaModel.from_pretrained('roberta-base')\n",
        "        self.d1 = torch.nn.Dropout(dropout_rate)\n",
        "        #self.d1 = torch.nn.Dropout(dropout_rate)\n",
        "        self.l1 = torch.nn.Linear(768, 32)\n",
        "        #self.l1 = torch.nn.Linear(768, 64)\n",
        "        #self.bn1 = torch.nn.LayerNorm(64)\n",
        "        self.bn1 = torch.nn.LayerNorm(32)\n",
        "        self.d2 = torch.nn.Dropout(dropout_rate)\n",
        "        #self.l2 = torch.nn.Linear(64, 2)\n",
        "        self.l2 = torch.nn.Linear(32, 2)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        #_, x = self.roberta(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        _, x = self.roberta(input_ids=input_ids, attention_mask=attention_mask, return_dict=False)\n",
        "        x = self.d1(x)\n",
        "        x = self.l1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = torch.nn.Tanh()(x)\n",
        "        x = self.d2(x)\n",
        "        x = self.l2(x)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "79sp1YVwD1nY"
      },
      "outputs": [],
      "source": [
        "def pretrain(model,\n",
        "             optimizer,\n",
        "             train_iter,\n",
        "             valid_iter,\n",
        "             scheduler = None,\n",
        "             valid_period = len(train_iter),\n",
        "             num_epochs = 5):\n",
        "\n",
        "    # Pretrain linear layers, do not train bert\n",
        "    for param in model.roberta.parameters():\n",
        "        param.requires_grad = False\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    # Initialize losses and loss histories\n",
        "    train_loss = 0.0\n",
        "    valid_loss = 0.0\n",
        "    global_step = 0\n",
        "\n",
        "    # Train loop\n",
        "    for epoch in range(num_epochs):\n",
        "        for (source, target), _ in train_iter:\n",
        "            mask = (source != PAD_INDEX).type(torch.uint8)\n",
        "\n",
        "            y_pred = model(input_ids=source,\n",
        "                           attention_mask=mask)\n",
        "\n",
        "            loss = torch.nn.CrossEntropyLoss()(y_pred, target)\n",
        "\n",
        "            loss.backward()\n",
        "\n",
        "            # Optimizer and scheduler step\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Update train loss and global step\n",
        "            train_loss += loss.item()\n",
        "            global_step += 1\n",
        "\n",
        "            # Validation loop. Save progress and evaluate model performance.\n",
        "            if global_step % valid_period == 0:\n",
        "                model.eval()\n",
        "\n",
        "                with torch.no_grad():\n",
        "                    for (source, target), _ in valid_iter:\n",
        "                        mask = (source != PAD_INDEX).type(torch.uint8)\n",
        "\n",
        "                        y_pred = model(input_ids=source,\n",
        "                                       attention_mask=mask)\n",
        "\n",
        "                        loss = torch.nn.CrossEntropyLoss()(y_pred, target)\n",
        "\n",
        "                        valid_loss += loss.item()\n",
        "\n",
        "                # Store train and validation loss history\n",
        "                train_loss = train_loss / valid_period\n",
        "                valid_loss = valid_loss / len(valid_iter)\n",
        "\n",
        "                model.train()\n",
        "\n",
        "                # print summary\n",
        "                print('Epoch [{}/{}], global step [{}/{}], PT Loss: {:.4f}, Val Loss: {:.4f}'\n",
        "                      .format(epoch+1, num_epochs, global_step, num_epochs*len(train_iter),\n",
        "                              train_loss, valid_loss))\n",
        "\n",
        "                train_loss = 0.0\n",
        "                valid_loss = 0.0\n",
        "\n",
        "    # Set bert parameters back to trainable\n",
        "    for param in model.roberta.parameters():\n",
        "        param.requires_grad = True\n",
        "\n",
        "    print('Pre-training done!')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WIISE4xDEBBu"
      },
      "outputs": [],
      "source": [
        "# Training Function\n",
        "\n",
        "def train(model,\n",
        "          optimizer,\n",
        "          train_iter,\n",
        "          valid_iter,\n",
        "          scheduler = None,\n",
        "          num_epochs = 5,\n",
        "          valid_period = len(train_iter),\n",
        "          output_path = output_path):\n",
        "\n",
        "    # Initialize losses and loss histories\n",
        "    train_loss = 0.0\n",
        "    valid_loss = 0.0\n",
        "    train_loss_list = []\n",
        "    valid_loss_list = []\n",
        "    best_valid_loss = float('Inf')\n",
        "\n",
        "    global_step = 0\n",
        "    global_steps_list = []\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    # Train loop\n",
        "    for epoch in range(num_epochs):\n",
        "        for (source, target), _ in train_iter:\n",
        "            mask = (source != PAD_INDEX).type(torch.uint8)\n",
        "\n",
        "            y_pred = model(input_ids=source,\n",
        "                           attention_mask=mask)\n",
        "            #output = model(input_ids=source,\n",
        "            #              labels=target,\n",
        "            #              attention_mask=mask)\n",
        "\n",
        "            loss = torch.nn.CrossEntropyLoss()(y_pred, target)\n",
        "            #loss = output[0]\n",
        "\n",
        "            loss.backward()\n",
        "\n",
        "            #torch.nn.utils.clip_grad_norm_(model.parameters(), 0.1)\n",
        "\n",
        "            # Optimizer and scheduler step\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Update train loss and global step\n",
        "            train_loss += loss.item()\n",
        "            global_step += 1\n",
        "\n",
        "            # Validation loop. Save progress and evaluate model performance.\n",
        "            if global_step % valid_period == 0:\n",
        "                model.eval()\n",
        "\n",
        "                with torch.no_grad():\n",
        "                    for (source, target), _ in valid_iter:\n",
        "                        mask = (source != PAD_INDEX).type(torch.uint8)\n",
        "\n",
        "                        y_pred = model(input_ids=source,\n",
        "                                       attention_mask=mask)\n",
        "                        #output = model(input_ids=source,\n",
        "                        #               labels=target,\n",
        "                        #               attention_mask=mask)\n",
        "\n",
        "                        loss = torch.nn.CrossEntropyLoss()(y_pred, target)\n",
        "                        #loss = output[0]\n",
        "\n",
        "                        valid_loss += loss.item()\n",
        "\n",
        "                # Store train and validation loss history\n",
        "                train_loss = train_loss / valid_period\n",
        "                valid_loss = valid_loss / len(valid_iter)\n",
        "                train_loss_list.append(train_loss)\n",
        "                valid_loss_list.append(valid_loss)\n",
        "                global_steps_list.append(global_step)\n",
        "\n",
        "                # print summary\n",
        "                print('Epoch [{}/{}], global step [{}/{}], Train Loss: {:.4f}, Valid Loss: {:.4f}'\n",
        "                      .format(epoch+1, num_epochs, global_step, num_epochs*len(train_iter),\n",
        "                              train_loss, valid_loss))\n",
        "\n",
        "                # checkpoint\n",
        "                if best_valid_loss > valid_loss:\n",
        "                    best_valid_loss = valid_loss\n",
        "                    save_checkpoint(output_path + '/model.pkl', model, best_valid_loss)\n",
        "                    save_metrics(output_path + '/metric.pkl', train_loss_list, valid_loss_list, global_steps_list)\n",
        "\n",
        "                train_loss = 0.0\n",
        "                valid_loss = 0.0\n",
        "                model.train()\n",
        "\n",
        "    save_metrics(output_path + '/metric.pkl', train_loss_list, valid_loss_list, global_steps_list)\n",
        "    print('Training done!')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tj1jxmI-EFyD"
      },
      "outputs": [],
      "source": [
        "# Main training loop\n",
        "NUM_EPOCHS = 20\n",
        "steps_per_epoch = len(train_iter)\n",
        "\n",
        "model = ROBERTAClassifier(0.4)\n",
        "model = model.to(device)\n",
        "\n",
        "\n",
        "optimizer = AdamW(model.parameters(), lr=1e-3)#lr=1e-4)\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer,\n",
        "                                            num_warmup_steps=steps_per_epoch*1,\n",
        "                                            num_training_steps=steps_per_epoch*NUM_EPOCHS)\n",
        "\n",
        "print(\"======================= Start pretraining ==============================\")\n",
        "\n",
        "pretrain(model=model,\n",
        "         train_iter=train_iter,\n",
        "         valid_iter=valid_iter,\n",
        "         optimizer=optimizer,\n",
        "         scheduler=scheduler,\n",
        "         num_epochs=NUM_EPOCHS)\n",
        "\n",
        "NUM_EPOCHS = 20\n",
        "print(\"======================= Start training =================================\")\n",
        "optimizer = AdamW(model.parameters(), lr=2e-6)#lr=2e-6)\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer,\n",
        "                                            num_warmup_steps=steps_per_epoch*2,\n",
        "                                            num_training_steps=steps_per_epoch*NUM_EPOCHS)\n",
        "\n",
        "train(model=model,\n",
        "      train_iter=train_iter,\n",
        "      valid_iter=valid_iter,\n",
        "      optimizer=optimizer,\n",
        "      scheduler=scheduler,\n",
        "      num_epochs=NUM_EPOCHS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cr5ruq_sEKI9"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10, 8))\n",
        "train_loss_list, valid_loss_list, global_steps_list = load_metrics(output_path + '/metric.pkl')\n",
        "plt.plot(global_steps_list, train_loss_list, label='Train')\n",
        "plt.plot(global_steps_list, valid_loss_list, label='Valid')\n",
        "plt.xlabel('Global Steps', fontsize=14)\n",
        "plt.ylabel('Loss', fontsize=14)\n",
        "plt.legend(fontsize=14)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H7_r3JvAEO-E"
      },
      "outputs": [],
      "source": [
        "# Evaluation Function\n",
        "Y_PRED = []\n",
        "\n",
        "def evaluate(model, test_loader):\n",
        "    y_pred = []\n",
        "    y_true = []\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for (source, target), _ in test_loader:\n",
        "                mask = (source != PAD_INDEX).type(torch.uint8)\n",
        "\n",
        "                output = model(source, attention_mask=mask)\n",
        "\n",
        "                y_pred.extend(torch.argmax(output, axis=-1).tolist())\n",
        "                y_true.extend(target.tolist())\n",
        "\n",
        "    print('Classification Report:')\n",
        "    print(classification_report(y_true, y_pred, labels=[1,0], digits=4))\n",
        "    Y_PRED = y_pred\n",
        "    cm = confusion_matrix(y_true, y_pred, labels=[1,0])\n",
        "    ax = plt.subplot()\n",
        "\n",
        "    sns.heatmap(cm, annot=True, ax = ax, cmap='Blues', fmt=\"d\")\n",
        "\n",
        "    ax.set_title('Confusion Matrix')\n",
        "\n",
        "    ax.set_xlabel('Predicted Labels')\n",
        "    ax.set_ylabel('True Labels')\n",
        "\n",
        "    ax.xaxis.set_ticklabels(['POSITIVE', 'NEGATIVE'])\n",
        "    ax.yaxis.set_ticklabels(['POSITIVE', 'NEGATIVE'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uoElaZFEESEP"
      },
      "outputs": [],
      "source": [
        "model = ROBERTAClassifier()\n",
        "model = model.to(device)\n",
        "\n",
        "load_checkpoint(output_path + '/model.pkl', model)\n",
        "\n",
        "evaluate(model, test_iter)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VKGy2ePLEVGY"
      },
      "outputs": [],
      "source": [
        "print(len(train_data))\n",
        "print(len(valid_data))\n",
        "print(len(test_data))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OlS38UKE9UK5"
      },
      "outputs": [],
      "source": [
        "!pip install graphviz\n",
        "!pip install hiddenlayer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "epsxEXebAZS7"
      },
      "outputs": [],
      "source": [
        "#import hiddenlayer as hl\n",
        "#graph = hl.build_graph(model, model.named_parameters())\n",
        "#graph = graph.build_dot()\n",
        "#graph.render(output_path, view=True, format='png')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YqexoUC79AE-"
      },
      "outputs": [],
      "source": [
        "import hiddenlayer as hl\n",
        "\n",
        "#transforms = [ hl.transforms.Prune('Constant') ] # Removes Constant nodes from graph.\n",
        "\n",
        "#graph = hl.build_graph( model, batch.text, transforms=transforms)\n",
        "#hl.build_graph(model, torch.zeros([1, 3, 224, 224]))\n",
        "#graph.theme = hl.graph.THEMES['blue'].copy()\n",
        "#graph.save('rnn_hiddenlayer', format='png')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lEc5Mzy52hZI"
      },
      "outputs": [],
      "source": [
        "!pip install graphviz\n",
        "!pip install torchviz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OQbypEmwJP0_"
      },
      "outputs": [],
      "source": [
        "type(Y_PRED)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sqXn88VE2rc7"
      },
      "outputs": [],
      "source": [
        "#from torchviz import make_dot\n",
        "#make_dot(model).render(\"detached\", format=\"png\")\n",
        "\n",
        "#make_dot(Y_PRED, params=dict(list(model.named_parameters()))).render(\"rnn_torchviz\", format=\"png\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "p=0.81\n",
        "r=0.82\n",
        "print((2*p*r)/(p+r))"
      ],
      "metadata": {
        "id": "YAL_Db30eCZz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "p=0.82\n",
        "r=0.85\n",
        "print((2*p*r)/(p+r))"
      ],
      "metadata": {
        "id": "_zbW8rJve7M8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(0.86+0.82)/2"
      ],
      "metadata": {
        "id": "1avaAmX8faza"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(0.82+0.85)/2"
      ],
      "metadata": {
        "id": "tHj3osUcfkac"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(0.83+0.84)/2"
      ],
      "metadata": {
        "id": "GI7kRe3RfulY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pp=0.81\n",
        "rp=0.81"
      ],
      "metadata": {
        "id": "2S4yLKWLj1kP"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "authorship_tag": "ABX9TyPSSR78irSrzkXRmjjrasJx",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}