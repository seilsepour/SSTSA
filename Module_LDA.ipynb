{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "bFFuE5aegGC0",
        "i-liD7f3gWZn",
        "mN7XFl3JglsV",
        "5HQHaMFZjHgi",
        "o1-SOeExjWo8",
        "4XW4b9j3jbdl",
        "q7hsgmzD7RHi",
        "MjgXV8cSOnNd",
        "n2tC4S6XPMnB",
        "lcpzQ3uIVBgE",
        "Lf82LaH49m4N"
      ],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/seilsepour/SSTSA/blob/main/Module_LDA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "voMHZ8LdUkEL"
      },
      "source": [
        "https://github.com/Arghyadeep/Document-Classification-with-Unsupervised-Learning-and-Topic-Modelling/blob/master/topic%20modelling/diffbot%20article%20API%20test.ipynb\n",
        "\n",
        "https://argiri.wordpress.com/2018/11/17/topic-modelling-with-lda-and-word2vec/\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rn_KhEI3kQTX"
      },
      "source": [
        "Topic number = 8, 20, 40, 50, 60, 65, 70"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lwb4Ph9h9Na3"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4-SYWRsdmtmk"
      },
      "source": [
        "pip install pandas --upgrade"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sLhtaspD-vhl"
      },
      "source": [
        "#pip install pyLDAvis"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BFnu4UiB9QGp"
      },
      "source": [
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pprint import pprint\n",
        "\n",
        "# Gensim\n",
        "import gensim\n",
        "import gensim.corpora as corpora\n",
        "from gensim.utils import simple_preprocess\n",
        "from gensim.models import CoherenceModel\n",
        "\n",
        "# spacy for lemmatization\n",
        "#import spacy\n",
        "\n",
        "# Plotting tools\n",
        "#import pyLDAvis\n",
        "#import pyLDAvis.gensim  # don't skip this\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "# Enable logging for gensim - optional\n",
        "import logging\n",
        "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.ERROR)\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\",category=DeprecationWarning)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RiYTcq6lW7ey"
      },
      "source": [
        "## Configurations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zSc3eEj9UkEX"
      },
      "source": [
        "#DEFAULT_SAVING_PATH = 'D:\\\\Jupyter notebooks\\\\Twitter-Gensim-LDA-Labelling-WE-Edited2\\\\'\n",
        "#DATASET_PATH = 'D:\\\\Jupyter notebooks\\\\Twitter-Gensim-LDA-Labelling-WE-Edited2\\\\dataset-USA-stream.xlsx'\n",
        "DEFAULT_SAVING_PATH = '/MYDRIVE/My Drive/Colab Notebooks/SSTSA/data/'\n",
        "OUTPUT_PATH = '/MYDRIVE/My Drive/Colab Notebooks/SSTSA/data/'\n",
        "#BIGRAM_PKL = 'Output\\\\bigram-uk-2000.pkl'\n",
        "#TRIGRAM_PKL = 'Output\\\\trigram-uk-2000.pkl'\n",
        "TOPICS_PKL = '/MYDRIVE/My Drive/Colab Notebooks/SSTSA/data/topics-jmwga.pkl'\n",
        "#TRIGRAM_OUTPUT = 'Output\\\\trigrams.csv'\n",
        "#BIGRAM_OUTPUT = 'Output\\\\bigrams.csv'\n",
        "TOP_WORDS_OUTPUT = '/MYDRIVE/My Drive/Colab Notebooks/SSTSA/data/toptweetwords-jmwga.csv'\n",
        "T_K_D = '/MYDRIVE/My Drive/Colab Notebooks/SSTSA/data/topkd-jmwga.csv'\n",
        "\n",
        "LDA_MODEL = '/MYDRIVE/My Drive/Colab Notebooks/SSTSA/data/model-jmwga.gensim'\n",
        "DIC_PATH = '/MYDRIVE/My Drive/Colab Notebooks/SSTSA/data/dictionary-jmwga.gensim'\n",
        "CORPUS_PATH = '/MYDRIVE/My Drive/Colab Notebooks/SSTSA/data/corpus-jmwga.pkl'\n",
        "LDA_TOPICS = '/MYDRIVE/My Drive/Colab Notebooks/SSTSA/data/lda-topics-jmwga.csv'\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Tweets_Topics_8 = '/MYDRIVE/My Drive/Colab Notebooks/SSTSA/data/Tweets_Topics_8.csv'\n",
        "Topics_Docs_Count_8 = '/MYDRIVE/My Drive/Colab Notebooks/SSTSA/data/Topics_Docs_Count_8.csv'\n",
        "Topics_8 = '/MYDRIVE/My Drive/Colab Notebooks/SSTSA/data/Topics_8.csv'\n",
        "\n",
        "Tweets_Topics_20 = '/MYDRIVE/My Drive/Colab Notebooks/SSTSA/data/Tweets_Topics_20.csv'\n",
        "Topics_Docs_Count_20 = '/MYDRIVE/My Drive/Colab Notebooks/SSTSA/data/Topics_Docs_Count_20.csv'\n",
        "Topics_20 = '/MYDRIVE/My Drive/Colab Notebooks/SSTSA/data/Topics_20.csv'\n",
        "\n",
        "Tweets_Topics_40 = '/MYDRIVE/My Drive/Colab Notebooks/SSTSA/data/Tweets_Topics_40.csv'\n",
        "Topics_Docs_Count_40 = '/MYDRIVE/My Drive/Colab Notebooks/SSTSA/data/Topics_Docs_Count_40.csv'\n",
        "Topics_40 = '/MYDRIVE/My Drive/Colab Notebooks/SSTSA/data/Topics_40.csv'\n",
        "\n",
        "Tweets_Topics_80 = '/MYDRIVE/My Drive/Colab Notebooks/SSTSA/data/Tweets_Topics_80.csv'\n",
        "Topics_Docs_Count_80 = '/MYDRIVE/My Drive/Colab Notebooks/SSTSA/data/Topics_Docs_Count_80.csv'\n",
        "Topics_80 = '/MYDRIVE/My Drive/Colab Notebooks/SSTSA/data/Topics_80.csv'\n",
        "\n",
        "Tweets_Topics_60 = '/MYDRIVE/My Drive/Colab Notebooks/SSTSA/data/Tweets_Topics_60.csv'\n",
        "Topics_Docs_Count_60 = '/MYDRIVE/My Drive/Colab Notebooks/SSTSA/data/Topics_Docs_Count_60.csv'\n",
        "Topics_60 = '/MYDRIVE/My Drive/Colab Notebooks/SSTSA/data/Topics_60.csv'\n",
        "\n",
        "Tweets_Topics_65 = '/MYDRIVE/My Drive/Colab Notebooks/SSTSA/data/Tweets_Topics_65.csv'\n",
        "Topics_Docs_Count_65 = '/MYDRIVE/My Drive/Colab Notebooks/SSTSA/data/Topics_Docs_Count_65.csv'\n",
        "Topics_65 = '/MYDRIVE/My Drive/Colab Notebooks/SSTSA/data/Topics_65.csv'\n",
        "\n",
        "Tweets_Topics_50 = '/MYDRIVE/My Drive/Colab Notebooks/SSTSA/data/Tweets_Topics_50.csv'\n",
        "Topics_Docs_Count_50 = '/MYDRIVE/My Drive/Colab Notebooks/SSTSA/data/Topics_Docs_Count_50.csv'\n",
        "Topics_50 = '/MYDRIVE/My Drive/Colab Notebooks/SSTSA/data/Topics_50.csv'\n",
        "\n",
        "Tweets_Topics_70 = '/MYDRIVE/My Drive/Colab Notebooks/SSTSA/data/Tweets_Topics_70.csv'\n",
        "Topics_Docs_Count_70 = '/MYDRIVE/My Drive/Colab Notebooks/SSTSA/data/Topics_Docs_Count_70.csv'\n",
        "Topics_70 = '/MYDRIVE/My Drive/Colab Notebooks/SSTSA/data/Topics_70.csv'\n",
        "\n",
        "\n",
        "\n",
        "Top_Topic = '/MYDRIVE/My Drive/Colab Notebooks/SSTSA/data/Selected_Topic_Docs_99_09_14.csv'\n",
        "\n",
        "\n",
        "\n",
        "#preprocessedData = '/MYDRIVE/My Drive/Colab Notebooks/SSTSA/data/MovieReview-Pang-Ver2-processed-99-09-10.csv'\n",
        "#AMAZON-books-all-2.xlsx\n",
        "preprocessedData = '/MYDRIVE/My Drive/Colab Notebooks/SSTSA/data/PreProcessed-electronics-all-for-LDA.csv'\n",
        "\n",
        "\n",
        "NUM_TOPICS1 = 8\n",
        "NUM_TOPICS2 = 20\n",
        "NUM_TOPICS3 = 40\n",
        "NUM_TOPICS4 = 80\n",
        "NUM_TOPICS5 = 50\n",
        "NUM_TOPICS6 = 60\n",
        "NUM_TOPICS7 = 65\n",
        "NUM_TOPICS8 = 70\n",
        "\n",
        "\n",
        "NUM_WORDS = 15\n",
        "'''\n",
        "IN moteghayer moshakhas minamayad ke yek topic bayad dar che\n",
        "tedad doc vojud dashte bashad ta be onvane topice mohem ghalamdad shavad\n",
        "'''\n",
        "NUM_TOPIC_DOC = 50\n",
        "NUM_IMPORTANT_TOPICS = 100\n",
        "TWEET_CONTENT_LIMIT = 50\n",
        "\n",
        "\n",
        "TWEET_CONTENT = 'Tweet content'\n",
        "TWEET_LANG = 'lang'\n",
        "TWEET_ID = 'Tweet Id'\n",
        "TWEET_FOLLOWERS = 'Followers'\n",
        "TWEET_FOLLOWING = 'Following'\n",
        "TWEET_FAVS = 'Favs'\n",
        "TWEET_RTS = 'RTs'\n",
        "TWEET_LISTED = 'Listed'\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CusXw5IIUkEp"
      },
      "source": [
        "#MODEL_PATH_WIKIP = 'Models\\\\d2v-wiki-pretrained\\\\docvecmodel.d2v'\n",
        "#modelPathWiki = 'D:\\\\Jupyter notebooks\\\\Twitter-Gensim-LDA-Labelling-WE-Edited2\\\\doc2vec\\\\d2v-wp-2000000-1-model'\n",
        "\n",
        "#'/MYDRIVE/My Drive/Colab Notebooks/SSTSA/data/airline-processed-98-11-08.csv'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J_J3O_I0UkE4"
      },
      "source": [
        "import pprint\n",
        "import csv\n",
        "import time\n",
        "from gensim import corpora, models, similarities\n",
        "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
        "from itertools import chain\n",
        "from collections import Counter\n",
        "#import spacy\n",
        "import gensim, logging\n",
        "from time import time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import string\n",
        "from gensim.models import Word2Vec\n",
        "import pickle\n",
        "\n",
        "import nltk\n",
        "from nltk.collocations import *\n",
        "\n",
        "from gensim.models import Phrases\n",
        "from gensim.models.phrases import Phraser\n",
        "\n",
        "\n",
        "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r8XXKXVqkiAT"
      },
      "source": [
        "# Colab"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RsH5WnWwW3Bb"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/MYDRIVE', force_remount=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eOk_44PcXC2_"
      },
      "source": [
        "## Loading Files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "woD6DddypdD3"
      },
      "source": [
        "preprocessedData"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y8k_5BSqUkFG"
      },
      "source": [
        "#Getting Ready for LDA\n",
        "print(\"Getting Ready for LDA\")\n",
        "print(\"Loading preprocessed tweets\")\n",
        "dfPreprocessedTweets = pd.read_csv(preprocessedData, header=None)\n",
        "#dfPreprocessedTweets = pd.read_excel(preprocessedData, header=1)\n",
        "\n",
        "print(\"The preprocessed tweets were loaded\")\n",
        "print(\"The shape of dataframe is\", dfPreprocessedTweets.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HEfl1M2zp2vR"
      },
      "source": [
        "dfPreprocessedTweets.head(3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0xks-FKkym0N"
      },
      "source": [
        "dfPreprocessedTweets.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RBPxSRyCyq3E"
      },
      "source": [
        "#dfPreprocessedTweets = dfPreprocessedTweets.dropna()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WWzsAIgyy0uh"
      },
      "source": [
        "dfPreprocessedTweets.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h6CrLAr2XY-q"
      },
      "source": [
        "# Making list before LDA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yN8W0Mo0ppFr"
      },
      "source": [
        "tweet_content = []\n",
        "print(\"Converting dataframe to array\")\n",
        "for row in dfPreprocessedTweets.itertuples(index=False):\n",
        "    tweet_content.append(row[1])\n",
        "print(\"Converting is done successfully\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f7lj9ht1zb9Q"
      },
      "source": [
        "tweet_content[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qtaGvP7JqjOP"
      },
      "source": [
        "tweet_content[5]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vNxabVG-qmkT"
      },
      "source": [
        "len(tweet_content)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d6B8FYG3xKaa"
      },
      "source": [
        "type(tweet_content)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zcWsAM3fzzgr"
      },
      "source": [
        "tweet_content = [d.split() for d in tweet_content]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JiPH8WllUkFp"
      },
      "source": [
        "# LDA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I7nVjCAJq1LT"
      },
      "source": [
        "DIC_PATH"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9vqhPvrIrNSW"
      },
      "source": [
        "CORPUS_PATH"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8W-DZ8b6UkFt"
      },
      "source": [
        "print(\"Creating dictionary...\")\n",
        "dictionary = corpora.Dictionary(tweet_content)\n",
        "print(\"Saving dictionary...\")\n",
        "dictionary.save( DIC_PATH )\n",
        "print(\"Creating corpus...\")\n",
        "corpus = [dictionary.doc2bow(text) for text in tweet_content]\n",
        "print(\"Saving corpus\")\n",
        "pickle.dump(corpus, open( CORPUS_PATH , 'wb'))\n",
        "print(\"Corpus saved\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(dictionary)"
      ],
      "metadata": {
        "id": "muOI5WngTh99"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nwVW1N2p_sER"
      },
      "source": [
        "# Get the best topic number"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EdBMAdkB_v4J"
      },
      "source": [
        "def compute_coherence_values(dictionary, corpus, texts, limit, start=2, step=3):\n",
        "    \"\"\"\n",
        "    Compute c_v coherence for various number of topics\n",
        "\n",
        "    Parameters:\n",
        "    ----------\n",
        "    dictionary : Gensim dictionary\n",
        "    corpus : Gensim corpus\n",
        "    texts : List of input texts\n",
        "    limit : Max num of topics\n",
        "\n",
        "    Returns:\n",
        "    -------\n",
        "    model_list : List of LDA topic models\n",
        "    coherence_values : Coherence values corresponding to the LDA model with respective number of topics\n",
        "    \"\"\"\n",
        "    coherence_values = []\n",
        "    model_list = []\n",
        "    for num_topics in range(start, limit, step):\n",
        "        #model = gensim.models.wrappers.LdaMallet(mallet_path, corpus=corpus, num_topics=num_topics, id2word=id2word)\n",
        "        model = models.ldamodel.LdaModel(corpus=corpus,\n",
        "                                    id2word=dictionary,\n",
        "                                    iterations=200,\n",
        "                                    num_topics=num_topics,\n",
        "                                    update_every=1,\n",
        "                                    chunksize=100,\n",
        "                                    passes=20,\n",
        "                                    alpha='auto',\n",
        "                                    per_word_topics=True)\n",
        "        model_list.append(model)\n",
        "        coherencemodel = CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence='c_v')\n",
        "        coherence_values.append(coherencemodel.get_coherence())\n",
        "\n",
        "    return model_list, coherence_values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "waKGlU_H_0zD"
      },
      "source": [
        "# Can take a long time to run.\n",
        "model_list, coherence_values = compute_coherence_values(dictionary=dictionary, corpus=corpus, texts=tweet_content, start=5, limit=120, step=5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_12gaO5x_8Ib"
      },
      "source": [
        "# Show graph\n",
        "\n",
        "limit=120; start=5; step=5;\n",
        "x = range(start, limit, step)\n",
        "plt.plot(x, coherence_values)\n",
        "plt.xlabel(\"Num Topics\")\n",
        "plt.ylabel(\"Coherence score\")\n",
        "plt.legend((\"coherence_values\"), loc='best')\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1zu2ozUJACPV"
      },
      "source": [
        "# Print the coherence scores\n",
        "\n",
        "for m, cv in zip(x, coherence_values):\n",
        "    print(\"Num Topics =\", m, \" has Coherence Value of\", round(cv, 4))\n",
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "H204PkAdzXku"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ByRyuBBSelC-"
      },
      "source": [
        "#Topic Number = 8"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V-dTzcWqnLFz"
      },
      "source": [
        "!pip uninstall pandas\n",
        "!pip install pandas==1.1.5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pbHyWQZReuDW"
      },
      "source": [
        "NUM_TOPICS1 = 8\n",
        "print(\"Building lda model...\")\n",
        "t0= time()\n",
        "lda_model_8 = models.ldamodel.LdaModel(\n",
        "                                    corpus=corpus,\n",
        "                                    id2word=dictionary,\n",
        "                                    iterations=100,\n",
        "                                    num_topics=NUM_TOPICS1,\n",
        "                                    update_every=None,\n",
        "                                    chunksize=100,\n",
        "                                    passes=20,\n",
        "                                    eta='auto',\n",
        "                                    eval_every = None,\n",
        "                                    per_word_topics=True,\n",
        "                                    random_state = 123)\n",
        "\n",
        "topics1 = lda_model_8.print_topics(num_topics = NUM_TOPICS1, num_words= NUM_WORDS)\n",
        "#topics1 = lda_model_1.print_topics(num_words=NUM_WORDS)\n",
        "df = pd.DataFrame(topics1, columns=['TopicNo', 'Topic'])\n",
        "\n",
        "keywords = []\n",
        "for i in range(0,8):\n",
        "  temp = lda_model_8.show_topic(i,NUM_WORDS)\n",
        "\n",
        "  k = []\n",
        "  for item in temp:\n",
        "    k.append(item[0])\n",
        "  keywords.append(k)\n",
        "\n",
        "print(keywords[0])\n",
        "df2 = df.assign(Keywords = keywords)\n",
        "df2.to_csv(Topics_8)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mTqDegWdfcn8"
      },
      "source": [
        "# Topic Number = 20"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "djMr1-vkfgM7"
      },
      "source": [
        "NUM_TOPICS1 = 20\n",
        "print(\"Building lda model...\")\n",
        "t0= time()\n",
        "lda_model_20 = models.ldamodel.LdaModel(\n",
        "                                    corpus=corpus,\n",
        "                                    id2word=dictionary,\n",
        "                                    iterations=100,\n",
        "                                    num_topics=NUM_TOPICS1,\n",
        "                                    update_every=None,\n",
        "                                    chunksize=100,\n",
        "                                    passes=20,\n",
        "                                    eta='auto',\n",
        "                                    eval_every = None,\n",
        "                                    per_word_topics=True,\n",
        "                                    random_state = 123)\n",
        "\n",
        "topics1 = lda_model_20.print_topics(num_topics = NUM_TOPICS1, num_words= NUM_WORDS)\n",
        "#topics1 = lda_model_1.print_topics(num_words=NUM_WORDS)\n",
        "df = pd.DataFrame(topics1, columns=['TopicNo', 'Topic'])\n",
        "\n",
        "keywords = []\n",
        "for i in range(0,20):\n",
        "  temp = lda_model_20.show_topic(i,NUM_WORDS)\n",
        "\n",
        "  k = []\n",
        "  for item in temp:\n",
        "    k.append(item[0])\n",
        "  keywords.append(k)\n",
        "\n",
        "print(keywords[0])\n",
        "df2 = df.assign(Keywords = keywords)\n",
        "df2.to_csv(Topics_20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bFFuE5aegGC0"
      },
      "source": [
        "# Topic Number = 40"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZSEU_QPYgKRa"
      },
      "source": [
        "NUM_TOPICS1 = 40\n",
        "print(\"Building lda model...\")\n",
        "t0= time()\n",
        "lda_model_40 = models.ldamodel.LdaModel(\n",
        "                                    corpus=corpus,\n",
        "                                    id2word=dictionary,\n",
        "                                    iterations=100,\n",
        "                                    num_topics=NUM_TOPICS1,\n",
        "                                    update_every=None,\n",
        "                                    chunksize=100,\n",
        "                                    passes=20,\n",
        "                                    eta='auto',\n",
        "                                    eval_every = None,\n",
        "                                    per_word_topics=True,\n",
        "                                    random_state = 123)\n",
        "\n",
        "topics1 = lda_model_40.print_topics(num_topics = NUM_TOPICS1, num_words= NUM_WORDS)\n",
        "#topics1 = lda_model_1.print_topics(num_words=NUM_WORDS)\n",
        "df = pd.DataFrame(topics1, columns=['TopicNo', 'Topic'])\n",
        "\n",
        "keywords = []\n",
        "for i in range(0,40):\n",
        "  temp = lda_model_40.show_topic(i,NUM_WORDS)\n",
        "\n",
        "  k = []\n",
        "  for item in temp:\n",
        "    k.append(item[0])\n",
        "  keywords.append(k)\n",
        "\n",
        "print(keywords[0])\n",
        "df2 = df.assign(Keywords = keywords)\n",
        "df2.to_csv(Topics_40)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i-liD7f3gWZn"
      },
      "source": [
        "# Topic Number = 80"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4QSYZo85gb66"
      },
      "source": [
        "NUM_TOPICS1 = 80\n",
        "print(\"Building lda model...\")\n",
        "t0= time()\n",
        "lda_model_80 = models.ldamodel.LdaModel(\n",
        "                                    corpus=corpus,\n",
        "                                    id2word=dictionary,\n",
        "                                    iterations=100,\n",
        "                                    num_topics=NUM_TOPICS1,\n",
        "                                    update_every=None,\n",
        "                                    chunksize=100,\n",
        "                                    passes=20,\n",
        "                                    eta='auto',\n",
        "                                    eval_every = None,\n",
        "                                    per_word_topics=True,\n",
        "                                    random_state = 123)\n",
        "\n",
        "topics1 = lda_model_80.print_topics(num_topics = NUM_TOPICS1, num_words= NUM_WORDS)\n",
        "#topics1 = lda_model_1.print_topics(num_words=NUM_WORDS)\n",
        "df = pd.DataFrame(topics1, columns=['TopicNo', 'Topic'])\n",
        "\n",
        "keywords = []\n",
        "for i in range(0,80):\n",
        "  temp = lda_model_80.show_topic(i,NUM_WORDS)\n",
        "\n",
        "  k = []\n",
        "  for item in temp:\n",
        "    k.append(item[0])\n",
        "  keywords.append(k)\n",
        "\n",
        "print(keywords[0])\n",
        "df2 = df.assign(Keywords = keywords)\n",
        "df2.to_csv(Topics_80)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mN7XFl3JglsV"
      },
      "source": [
        "# Topic Number = 50"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q4TeKz3ogrmK"
      },
      "source": [
        "NUM_TOPICS1 = 50\n",
        "print(\"Building lda model...\")\n",
        "t0= time()\n",
        "lda_model_50 = models.ldamodel.LdaModel(\n",
        "                                    corpus=corpus,\n",
        "                                    id2word=dictionary,\n",
        "                                    iterations=100,\n",
        "                                    num_topics=NUM_TOPICS1,\n",
        "                                    update_every=None,\n",
        "                                    chunksize=100,\n",
        "                                    passes=20,\n",
        "                                    eta='auto',\n",
        "                                    eval_every = None,\n",
        "                                    per_word_topics=True,\n",
        "                                    random_state = 123)\n",
        "\n",
        "topics1 = lda_model_50.print_topics(num_topics = NUM_TOPICS1, num_words= NUM_WORDS)\n",
        "#topics1 = lda_model_1.print_topics(num_words=NUM_WORDS)\n",
        "df = pd.DataFrame(topics1, columns=['TopicNo', 'Topic'])\n",
        "\n",
        "keywords = []\n",
        "for i in range(0,50):\n",
        "  temp = lda_model_50.show_topic(i,NUM_WORDS)\n",
        "\n",
        "  k = []\n",
        "  for item in temp:\n",
        "    k.append(item[0])\n",
        "  keywords.append(k)\n",
        "\n",
        "print(keywords[0])\n",
        "df2 = df.assign(Keywords = keywords)\n",
        "df2.to_csv(Topics_50)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5HQHaMFZjHgi"
      },
      "source": [
        "# Topic Number = 60"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rxgGGkRmjLaX"
      },
      "source": [
        "NUM_TOPICS1 = 60\n",
        "print(\"Building lda model...\")\n",
        "t0= time()\n",
        "lda_model_60 = models.ldamodel.LdaModel(\n",
        "                                    corpus=corpus,\n",
        "                                    id2word=dictionary,\n",
        "                                    iterations=100,\n",
        "                                    num_topics=NUM_TOPICS1,\n",
        "                                    update_every=None,\n",
        "                                    chunksize=100,\n",
        "                                    passes=20,\n",
        "                                    eta='auto',\n",
        "                                    eval_every = None,\n",
        "                                    per_word_topics=True,\n",
        "                                    random_state = 123)\n",
        "\n",
        "topics1 = lda_model_60.print_topics(num_topics = NUM_TOPICS1, num_words= NUM_WORDS)\n",
        "#topics1 = lda_model_1.print_topics(num_words=NUM_WORDS)\n",
        "df = pd.DataFrame(topics1, columns=['TopicNo', 'Topic'])\n",
        "\n",
        "keywords = []\n",
        "for i in range(0,60):\n",
        "  temp = lda_model_60.show_topic(i,NUM_WORDS)\n",
        "\n",
        "  k = []\n",
        "  for item in temp:\n",
        "    k.append(item[0])\n",
        "  keywords.append(k)\n",
        "\n",
        "print(keywords[0])\n",
        "df2 = df.assign(Keywords = keywords)\n",
        "df2.to_csv(Topics_60)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o1-SOeExjWo8"
      },
      "source": [
        "# Topic Number  = 65"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E5UvdDF8jjLI"
      },
      "source": [
        "NUM_TOPICS1 = 65\n",
        "print(\"Building lda model...\")\n",
        "t0= time()\n",
        "lda_model_65 = models.ldamodel.LdaModel(\n",
        "                                    corpus=corpus,\n",
        "                                    id2word=dictionary,\n",
        "                                    iterations=100,\n",
        "                                    num_topics=NUM_TOPICS1,\n",
        "                                    update_every=None,\n",
        "                                    chunksize=100,\n",
        "                                    passes=20,\n",
        "                                    eta='auto',\n",
        "                                    eval_every = None,\n",
        "                                    per_word_topics=True,\n",
        "                                    random_state = 123)\n",
        "\n",
        "topics1 = lda_model_65.print_topics(num_topics = NUM_TOPICS1, num_words= NUM_WORDS)\n",
        "#topics1 = lda_model_1.print_topics(num_words=NUM_WORDS)\n",
        "df = pd.DataFrame(topics1, columns=['TopicNo', 'Topic'])\n",
        "\n",
        "keywords = []\n",
        "for i in range(0,65):\n",
        "  temp = lda_model_65.show_topic(i,NUM_WORDS)\n",
        "\n",
        "  k = []\n",
        "  for item in temp:\n",
        "    k.append(item[0])\n",
        "  keywords.append(k)\n",
        "\n",
        "print(keywords[0])\n",
        "df2 = df.assign(Keywords = keywords)\n",
        "df2.to_csv(Topics_65)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4XW4b9j3jbdl"
      },
      "source": [
        "# Topic Number = 70"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h6dcd6cmjfah"
      },
      "source": [
        "NUM_TOPICS1 = 70\n",
        "print(\"Building lda model...\")\n",
        "t0= time()\n",
        "lda_model_70 = models.ldamodel.LdaModel(\n",
        "                                    corpus=corpus,\n",
        "                                    id2word=dictionary,\n",
        "                                    iterations=100,\n",
        "                                    num_topics=NUM_TOPICS1,\n",
        "                                    update_every=None,\n",
        "                                    chunksize=100,\n",
        "                                    passes=20,\n",
        "                                    eta='auto',\n",
        "                                    eval_every = None,\n",
        "                                    per_word_topics=True,\n",
        "                                    random_state = 123)\n",
        "\n",
        "topics1 = lda_model_70.print_topics(num_topics = NUM_TOPICS1, num_words= NUM_WORDS)\n",
        "#topics1 = lda_model_1.print_topics(num_words=NUM_WORDS)\n",
        "df = pd.DataFrame(topics1, columns=['TopicNo', 'Topic'])\n",
        "\n",
        "keywords = []\n",
        "for i in range(0,70):\n",
        "  temp = lda_model_70.show_topic(i,NUM_WORDS)\n",
        "\n",
        "  k = []\n",
        "  for item in temp:\n",
        "    k.append(item[0])\n",
        "  keywords.append(k)\n",
        "\n",
        "print(keywords[0])\n",
        "df2 = df.assign(Keywords = keywords)\n",
        "df2.to_csv(Topics_70)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0XVAJDC6jUrT"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q7hsgmzD7RHi"
      },
      "source": [
        "# LDA (Topic number = 8)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9AskFs432JgS"
      },
      "source": [
        "NUM_TOPICS1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o2AEbwDDUkF8"
      },
      "source": [
        "print(\"Building lda model...\")\n",
        "t0= time()\n",
        "lda_model_1 = models.ldamodel.LdaModel(corpus=corpus,\n",
        "                                    id2word=dictionary,\n",
        "                                    iterations=200,\n",
        "                                    num_topics=NUM_TOPICS1,\n",
        "                                    update_every=1,\n",
        "                                    chunksize=100,\n",
        "                                    passes=20,\n",
        "                                    alpha='auto',\n",
        "                                    per_word_topics=True)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2KTKxpIS9iLj"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "## Preplexity and Coherence"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OQCXOEXE8uxs"
      },
      "source": [
        "# Compute Perplexity\n",
        "print('\\nPerplexity: ', lda_model_1.log_perplexity(corpus))  # a measure of how good the model is. lower the better.\n",
        "\n",
        "# Compute Coherence Score\n",
        "coherence_model_lda_1 = CoherenceModel(model=lda_model_1, texts=tweet_content, dictionary=dictionary, coherence='c_v')\n",
        "coherence_lda_1 = coherence_model_lda_1.get_coherence()\n",
        "print('\\nCoherence Score: ', coherence_lda_1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xOZeqjVrgIcv"
      },
      "source": [
        "NUM_WORDS"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8cT9nGB9-6lp"
      },
      "source": [
        "##Finding Topics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BCIh8bVF-8_z"
      },
      "source": [
        "topics1 = lda_model_1.print_topics(num_topics = NUM_TOPICS1, num_words= 15)\n",
        "#topics1 = lda_model_1.print_topics(num_words=NUM_WORDS)\n",
        "df = pd.DataFrame(topics1, columns=['TopicNo', 'Topic'])\n",
        "\n",
        "keywords = []\n",
        "for i in range(0,8):\n",
        "  temp = lda_model_1.show_topic(i,15)\n",
        "\n",
        "  k = []\n",
        "  for item in temp:\n",
        "    k.append(item[0])\n",
        "  keywords.append(k)\n",
        "\n",
        "print(keywords[0])\n",
        "df2 = df.assign(Keywords = keywords)\n",
        "df2.to_csv(Topics_8)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IZSwGi7OqJc0"
      },
      "source": [
        "df2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DOyJwEmBCjP3"
      },
      "source": [
        "topics1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QmAjnH6V_PDC"
      },
      "source": [
        "# Visualize the topics\n",
        "pyLDAvis.enable_notebook()\n",
        "vis = pyLDAvis.gensim.prepare(lda_model_1, corpus, dictionary)\n",
        "vis"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0iG4lQNs8_xj"
      },
      "source": [
        "LDA_MODEL"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "76z2P6VQUkGM"
      },
      "source": [
        "topics1 = lda_model_1.print_topics(num_topics = NUM_TOPICS1, num_words= 15)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pIrb67K9DkxA"
      },
      "source": [
        "NUM_WORDS"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "26VyEvW4Dnco"
      },
      "source": [
        "topics1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "viMDe0U9XlHt"
      },
      "source": [
        "## Saving topics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X54fgzboUkGd"
      },
      "source": [
        "def format_topics_sentences(ldamodel=lda_model_1, corpus=corpus, texts=tweet_content):\n",
        "    # Init output\n",
        "    sent_topics_df = pd.DataFrame()\n",
        "\n",
        "    # Get main topic in each document\n",
        "    for i, row in enumerate(ldamodel[corpus]):\n",
        "        row = sorted(row[0], key=lambda x: (x[1]), reverse=True)\n",
        "        # Get the Dominant topic, Perc Contribution and Keywords for each document\n",
        "        for j, (topic_num, prop_topic) in enumerate(row):\n",
        "            if j == 0:  # => dominant topic\n",
        "                wp = ldamodel.show_topic(topic_num)\n",
        "                topic_keywords = \", \".join([word for word, prop in wp])\n",
        "                sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n",
        "            else:\n",
        "                break\n",
        "    sent_topics_df.columns = ['Dominant_Topic', 'Perc_Contribution', 'Topic_Keywords']\n",
        "\n",
        "    # Add original text to the end of the output\n",
        "    contents = pd.Series(texts)\n",
        "    sent_topics_df = pd.concat([sent_topics_df, contents], axis=1)\n",
        "\n",
        "    return(sent_topics_df)\n",
        "\n",
        "\n",
        "df_topic_sents_keywords1 = format_topics_sentences(ldamodel=lda_model_1, corpus=corpus, texts=tweet_content)\n",
        "\n",
        "# Format\n",
        "df_dominant_topic1 = df_topic_sents_keywords1.reset_index()\n",
        "df_dominant_topic1.columns = ['Document_No', 'Dominant_Topic', 'Topic_Perc_Contrib', 'Keywords', 'Text']\n",
        "\n",
        "# Show\n",
        "#df_dominant_topic.head(NUM_TOPICS)\n",
        "\n",
        "df_dominant_topic1.head(100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_j4sUz-fG6Ub"
      },
      "source": [
        "df_dominant_topic1.head(200)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZFto9SEzDLdQ"
      },
      "source": [
        "df_dominant_topic1.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uTOiG2kfEa5R"
      },
      "source": [
        "df_dominant_topic1.to_csv(Tweets_Topics_8)\n",
        "Tweets_Topics_8"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eHYnUG8ay9v6"
      },
      "source": [
        "len(df_dominant_topic1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EWjxCUkMUkGt"
      },
      "source": [
        "19. Find the most representative document for each topic\n",
        "Sometimes just the topic keywords may not be enough to make sense of what a topic is about. So, to help with understanding the topic, you can find the documents a given topic has contributed to the most and infer the topic by reading that document. Whew!!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PP7FSp1EUkGx"
      },
      "source": [
        "# Group top 5 sentences under each topic\n",
        "sent_topics_sorteddf_mallet1 = pd.DataFrame()\n",
        "\n",
        "sent_topics_outdf_grpd1 = df_topic_sents_keywords1.groupby('Dominant_Topic')\n",
        "print(len(df_topic_sents_keywords1))\n",
        "print(len(sent_topics_outdf_grpd1))\n",
        "\n",
        "for i, grp in sent_topics_outdf_grpd1:\n",
        "    sent_topics_sorteddf_mallet1 = pd.concat([sent_topics_sorteddf_mallet1,\n",
        "                                             grp.sort_values(['Perc_Contribution'], ascending=[0]).head(1)],\n",
        "                                            axis=0)\n",
        "\n",
        "# Reset Index\n",
        "sent_topics_sorteddf_mallet1.reset_index(drop=True, inplace=True)\n",
        "\n",
        "# Format\n",
        "sent_topics_sorteddf_mallet1.columns = ['Topic_Num', \"Topic_Perc_Contrib\", \"Keywords\", \"Text\"]\n",
        "\n",
        "df_top_doc1 = sent_topics_sorteddf_mallet1\n",
        "\n",
        "# Show\n",
        "sent_topics_sorteddf_mallet1.head(NUM_TOPICS1)\n",
        "print(sent_topics_sorteddf_mallet1.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U3tPqSSiUkG-"
      },
      "source": [
        "20. Topic distribution across documents\n",
        "Finally, we want to understand the volume and distribution of topics in order to judge how widely it was discussed. The below table exposes that information."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jMG7Ti7AUkHD"
      },
      "source": [
        "# Number of Documents for Each Topic\n",
        "topic_counts1 = df_topic_sents_keywords1['Dominant_Topic'].value_counts()\n",
        "df_top_doc_count1 =  topic_counts1\n",
        "'''\n",
        "# Percentage of Documents for Each Topic\n",
        "topic_contribution = round(topic_counts/topic_counts.sum(), 4)\n",
        "\n",
        "# Topic Number and Keywords\n",
        "topic_num_keywords = df_topic_sents_keywords[['Dominant_Topic', 'Topic_Keywords']]\n",
        "\n",
        "# Concatenate Column wise\n",
        "df_dominant_topics = pd.concat([topic_num_keywords, topic_counts, topic_contribution], axis=1)\n",
        "\n",
        "# Change Column names\n",
        "df_dominant_topics.columns = ['Dominant_Topic', 'Topic_Keywords', 'Num_Documents', 'Perc_Documents']\n",
        "\n",
        "# Show\n",
        "df_dominant_topics\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gKU3uLu1UkHQ"
      },
      "source": [
        "topic_counts1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "twWJjxPQUkHe"
      },
      "source": [
        "df_top_selected1 = df_top_doc_count1 [df_top_doc_count1 >= 0 ]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cNbtIsVQUkHr"
      },
      "source": [
        "df_top_selected1.index"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P7n1PwJpUkJN"
      },
      "source": [
        "in this step, I create a dataframe including Topic_Num, Keywords, Doc_Num"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oWjT_qI5UkJQ"
      },
      "source": [
        "df_t_k_d1 = pd.DataFrame(columns = ['Topic_Num','Doc_Num','Topic', 'Keywords'])\n",
        "#df_selected.columns = ['Topic_Num','Keywords','Doc_Num']\n",
        "for i, v in df_top_selected1.iteritems():\n",
        "    wp = lda_model_1.show_topic(int(i))\n",
        "    topic_keywords = \", \".join([word for word, prop in wp])\n",
        "    new_entry = {'Topic_Num': i, 'Doc_Num': v, 'Topic': lda_model_1.show_topic(int(i)),\n",
        "                'Keywords': topic_keywords }\n",
        "    #row = [i,v]\n",
        "    df_t_k_d1.loc[len(df_t_k_d1)] = new_entry\n",
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n4UJx2yIUkJb"
      },
      "source": [
        "df_t_k_d1.to_csv(Topics_Docs_Count_8)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hS4pKX89DtZP"
      },
      "source": [
        "df_t_k_d1.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MjgXV8cSOnNd"
      },
      "source": [
        "# LDA (Topic Number=20)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m0g7n1hhPMnA"
      },
      "source": [
        "NUM_TOPICS2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tqsBO_wNPMnB"
      },
      "source": [
        "print(\"Building lda model...\")\n",
        "t0= time()\n",
        "lda_model_2 = models.ldamodel.LdaModel(corpus=corpus,\n",
        "                                    id2word=dictionary,\n",
        "                                    iterations=200,\n",
        "                                    num_topics=NUM_TOPICS2,\n",
        "                                    update_every=1,\n",
        "                                    chunksize=100,\n",
        "                                    passes=20,\n",
        "                                    alpha='auto',\n",
        "                                    per_word_topics=True)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V_O3xQNFPMnB"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "## Preplexity and Coherence"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V-oYU7IpPMnB"
      },
      "source": [
        "# Compute Perplexity\n",
        "print('\\nPerplexity: ', lda_model_2.log_perplexity(corpus))  # a measure of how good the model is. lower the better.\n",
        "\n",
        "# Compute Coherence Score\n",
        "coherence_model_lda_2 = CoherenceModel(model=lda_model_2, texts=tweet_content, dictionary=dictionary, coherence='c_v')\n",
        "coherence_lda_2 = coherence_model_lda_2.get_coherence()\n",
        "print('\\nCoherence Score: ', coherence_lda_2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n2tC4S6XPMnB"
      },
      "source": [
        "##Finding Topics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wUH0G7kYPMnC"
      },
      "source": [
        "topics2 = lda_model_2.print_topics(num_topics = NUM_TOPICS2, num_words= 15)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0cYGCsnazZF7"
      },
      "source": [
        "topics2 = lda_model_2.print_topics(num_topics = NUM_TOPICS2, num_words= 15)\n",
        "df = pd.DataFrame(topics2, columns=['TopicNo', 'Topic'])\n",
        "\n",
        "keywords = []\n",
        "for i in range(0,20):\n",
        "  temp = lda_model_2.show_topic(i, 15)\n",
        "  k = []\n",
        "  for item in temp:\n",
        "    k.append(item[0])\n",
        "  keywords.append(k)\n",
        "\n",
        "print(keywords[0])\n",
        "df2 = df.assign(Keywords = keywords)\n",
        "df2.to_csv(Topics_20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NJPpeYWQzo4c"
      },
      "source": [
        "df2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9uHzDNqaPMnC"
      },
      "source": [
        "topics2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_a2U1IXUPMnC"
      },
      "source": [
        "# Visualize the topics\n",
        "pyLDAvis.enable_notebook()\n",
        "vis = pyLDAvis.gensim.prepare(lda_model_2, corpus, dictionary)\n",
        "vis"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ptOEdxU2PMnD"
      },
      "source": [
        "LDA_MODEL"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dlt1qSjoPMnD"
      },
      "source": [
        "topics2 = lda_model_2.print_topics(num_topics = NUM_TOPICS2, num_words= 15)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ur4BNJBIPMnD"
      },
      "source": [
        "NUM_WORDS"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pDIx55hpPMnD"
      },
      "source": [
        "topics2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wfzaSsoXPMnE"
      },
      "source": [
        "## Saving topics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_k2QIBSIPMnE"
      },
      "source": [
        "def format_topics_sentences2(ldamodel=lda_model_2, corpus=corpus, texts=tweet_content):\n",
        "    # Init output\n",
        "    sent_topics_df = pd.DataFrame()\n",
        "\n",
        "    # Get main topic in each document\n",
        "    for i, row in enumerate(ldamodel[corpus]):\n",
        "        row = sorted(row[0], key=lambda x: (x[1]), reverse=True)\n",
        "        # Get the Dominant topic, Perc Contribution and Keywords for each document\n",
        "        for j, (topic_num, prop_topic) in enumerate(row):\n",
        "            if j == 0:  # => dominant topic\n",
        "                wp = ldamodel.show_topic(topic_num)\n",
        "                topic_keywords = \", \".join([word for word, prop in wp])\n",
        "                sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n",
        "            else:\n",
        "                break\n",
        "    sent_topics_df.columns = ['Dominant_Topic', 'Perc_Contribution', 'Topic_Keywords']\n",
        "\n",
        "    # Add original text to the end of the output\n",
        "    contents = pd.Series(texts)\n",
        "    sent_topics_df = pd.concat([sent_topics_df, contents], axis=1)\n",
        "\n",
        "    return(sent_topics_df)\n",
        "\n",
        "\n",
        "df_topic_sents_keywords2 = format_topics_sentences2(ldamodel=lda_model_2, corpus=corpus, texts=tweet_content)\n",
        "\n",
        "# Format\n",
        "df_dominant_topic2 = df_topic_sents_keywords2.reset_index()\n",
        "df_dominant_topic2.columns = ['Document_No', 'Dominant_Topic', 'Topic_Perc_Contrib', 'Keywords', 'Text']\n",
        "\n",
        "# Show\n",
        "#df_dominant_topic.head(NUM_TOPICS)\n",
        "\n",
        "df_dominant_topic2.head(100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ki6Bu3dQPMnE"
      },
      "source": [
        "df_dominant_topic2.head(200)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vFuEuzsPPMnE"
      },
      "source": [
        "df_dominant_topic2.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pYqAtvtnPMnE"
      },
      "source": [
        "df_dominant_topic2.to_csv(Tweets_Topics_20)\n",
        "Tweets_Topics_20"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3kPH-d6ePMnE"
      },
      "source": [
        "19. Find the most representative document for each topic\n",
        "Sometimes just the topic keywords may not be enough to make sense of what a topic is about. So, to help with understanding the topic, you can find the documents a given topic has contributed to the most and infer the topic by reading that document. Whew!!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R7suX-7MPMnE"
      },
      "source": [
        "# Group top 5 sentences under each topic\n",
        "sent_topics_sorteddf_mallet2 = pd.DataFrame()\n",
        "\n",
        "sent_topics_outdf_grpd2 = df_topic_sents_keywords2.groupby('Dominant_Topic')\n",
        "print(len(df_topic_sents_keywords2))\n",
        "print(len(sent_topics_outdf_grpd2))\n",
        "\n",
        "for i, grp in sent_topics_outdf_grpd2:\n",
        "    sent_topics_sorteddf_mallet2 = pd.concat([sent_topics_sorteddf_mallet2,\n",
        "                                             grp.sort_values(['Perc_Contribution'], ascending=[0]).head(1)],\n",
        "                                            axis=0)\n",
        "\n",
        "# Reset Index\n",
        "sent_topics_sorteddf_mallet2.reset_index(drop=True, inplace=True)\n",
        "\n",
        "# Format\n",
        "sent_topics_sorteddf_mallet2.columns = ['Topic_Num', \"Topic_Perc_Contrib\", \"Keywords\", \"Text\"]\n",
        "\n",
        "df_top_doc2 = sent_topics_sorteddf_mallet2\n",
        "\n",
        "# Show\n",
        "sent_topics_sorteddf_mallet2.head(NUM_TOPICS2)\n",
        "print(sent_topics_sorteddf_mallet2.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HgrecgX1PMnF"
      },
      "source": [
        "20. Topic distribution across documents\n",
        "Finally, we want to understand the volume and distribution of topics in order to judge how widely it was discussed. The below table exposes that information."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KPzDNmf5PMnF"
      },
      "source": [
        "# Number of Documents for Each Topic\n",
        "topic_counts2 = df_topic_sents_keywords2['Dominant_Topic'].value_counts()\n",
        "df_top_doc_count2 =  topic_counts2\n",
        "'''\n",
        "# Percentage of Documents for Each Topic\n",
        "topic_contribution = round(topic_counts/topic_counts.sum(), 4)\n",
        "\n",
        "# Topic Number and Keywords\n",
        "topic_num_keywords = df_topic_sents_keywords[['Dominant_Topic', 'Topic_Keywords']]\n",
        "\n",
        "# Concatenate Column wise\n",
        "df_dominant_topics = pd.concat([topic_num_keywords, topic_counts, topic_contribution], axis=1)\n",
        "\n",
        "# Change Column names\n",
        "df_dominant_topics.columns = ['Dominant_Topic', 'Topic_Keywords', 'Num_Documents', 'Perc_Documents']\n",
        "\n",
        "# Show\n",
        "df_dominant_topics\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N_SOfRd5PMnF"
      },
      "source": [
        "topic_counts2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ga5NHbJPPMnF"
      },
      "source": [
        "df_top_selected2 = df_top_doc_count2 [df_top_doc_count2 >= 0 ]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "izbHF4gGPMnF"
      },
      "source": [
        "df_top_selected2.index"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UUZ-r2yAPMnF"
      },
      "source": [
        "in this step, I create a dataframe including Topic_Num, Keywords, Doc_Num"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8xzsas7WPMnF"
      },
      "source": [
        "df_t_k_d2 = pd.DataFrame(columns = ['Topic_Num','Doc_Num','Topic', 'Keywords'])\n",
        "#df_selected.columns = ['Topic_Num','Keywords','Doc_Num']\n",
        "for i, v in df_top_selected2.iteritems():\n",
        "    wp = lda_model_2.show_topic(int(i))\n",
        "    topic_keywords = \", \".join([word for word, prop in wp])\n",
        "    new_entry = {'Topic_Num': i, 'Doc_Num': v, 'Topic': lda_model_2.show_topic(int(i)),\n",
        "                'Keywords': topic_keywords }\n",
        "    #row = [i,v]\n",
        "    df_t_k_d2.loc[len(df_t_k_d2)] = new_entry\n",
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xo_h9Fe1PMnF"
      },
      "source": [
        "df_t_k_d2.to_csv(Topics_Docs_Count_20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XfTQV4mzPMnF"
      },
      "source": [
        "df_t_k_d2.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lcpzQ3uIVBgE"
      },
      "source": [
        "# LDA (Topic Number=40)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-SdSH0VDVBgE"
      },
      "source": [
        "NUM_TOPICS3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3A1xzFwPVBgG"
      },
      "source": [
        "print(\"Building lda model...\")\n",
        "t0= time()\n",
        "lda_model_3 = models.ldamodel.LdaModel(corpus=corpus,\n",
        "                                    id2word=dictionary,\n",
        "                                    iterations=200,\n",
        "                                    num_topics=NUM_TOPICS3,\n",
        "                                    update_every=1,\n",
        "                                    chunksize=100,\n",
        "                                    passes=20,\n",
        "                                    alpha='auto',\n",
        "                                    per_word_topics=True,\n",
        "                                   )\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IFc8bQ1XVBgG"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "## Preplexity and Coherence"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3C9mC3VmVBgG"
      },
      "source": [
        "# Compute Perplexity\n",
        "print('\\nPerplexity: ', lda_model_3.log_perplexity(corpus))  # a measure of how good the model is. lower the better.\n",
        "\n",
        "# Compute Coherence Score\n",
        "coherence_model_lda_3 = CoherenceModel(model=lda_model_3, texts=tweet_content, dictionary=dictionary, coherence='c_v')\n",
        "coherence_lda_3 = coherence_model_lda_3.get_coherence()\n",
        "print('\\nCoherence Score: ', coherence_lda_3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gqni8tipVBgG"
      },
      "source": [
        "##Finding Topics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N78kCeE4VBgG"
      },
      "source": [
        "topics3 = lda_model_3.print_topics(num_words=15)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zti7hHJo2Ix2"
      },
      "source": [
        "len(topics3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-jyED1Q2Xrtx"
      },
      "source": [
        "topics3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1koGWmyx3TRj"
      },
      "source": [
        "t = lda_model_3.show_topic(0,15)\n",
        "print(t)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2fApS2Bgz3tF"
      },
      "source": [
        "topics3 = lda_model_3.print_topics(num_topics = NUM_TOPICS3, num_words= 15)\n",
        "\n",
        "df = pd.DataFrame(topics3, columns=['TopicNo', 'Topic'])\n",
        "\n",
        "keywords = []\n",
        "for i in range(0,40):\n",
        "  temp = lda_model_3.show_topic(i, 15)\n",
        "  print(temp)\n",
        "  k = []\n",
        "  for item in temp:\n",
        "    k.append(item[0])\n",
        "  keywords.append(k)\n",
        "\n",
        "\n",
        "df2 = df.assign(Keywords = keywords)\n",
        "df2.to_csv(Topics_40)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qGZuYLY2VBgH"
      },
      "source": [
        "df2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FLpsQYifVBgH"
      },
      "source": [
        "# Visualize the topics\n",
        "pyLDAvis.enable_notebook()\n",
        "vis = pyLDAvis.gensim.prepare(lda_model_3, corpus, dictionary)\n",
        "vis"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wJfiz1WRVBgH"
      },
      "source": [
        "LDA_MODEL"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iw2WYyh_VBgI"
      },
      "source": [
        "topics3 = lda_model_3.print_topics(num_topics = NUM_TOPICS3, num_words= 15)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "clnibLEBVBgI"
      },
      "source": [
        "NUM_WORDS = 15"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wlhjo-xOVBgI"
      },
      "source": [
        "topics3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ENEF08bVBgI"
      },
      "source": [
        "## Saving topics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oZalDuU8VBgI"
      },
      "source": [
        "def format_topics_sentences3(ldamodel=lda_model_3, corpus=corpus, texts=tweet_content):\n",
        "    # Init output\n",
        "    sent_topics_df = pd.DataFrame()\n",
        "\n",
        "    # Get main topic in each document\n",
        "    for i, row in enumerate(ldamodel[corpus]):\n",
        "        row = sorted(row[0], key=lambda x: (x[1]), reverse=True)\n",
        "        # Get the Dominant topic, Perc Contribution and Keywords for each document\n",
        "        for j, (topic_num, prop_topic) in enumerate(row):\n",
        "            if j == 0:  # => dominant topic\n",
        "                wp = ldamodel.show_topic(topic_num)\n",
        "                topic_keywords = \", \".join([word for word, prop in wp])\n",
        "                sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n",
        "            else:\n",
        "                break\n",
        "    sent_topics_df.columns = ['Dominant_Topic', 'Perc_Contribution', 'Topic_Keywords']\n",
        "\n",
        "    # Add original text to the end of the output\n",
        "    contents = pd.Series(texts)\n",
        "    sent_topics_df = pd.concat([sent_topics_df, contents], axis=1)\n",
        "\n",
        "    return(sent_topics_df)\n",
        "\n",
        "\n",
        "df_topic_sents_keywords3 = format_topics_sentences3(ldamodel=lda_model_3, corpus=corpus, texts=tweet_content)\n",
        "\n",
        "# Format\n",
        "df_dominant_topic3 = df_topic_sents_keywords3.reset_index()\n",
        "df_dominant_topic3.columns = ['Document_No', 'Dominant_Topic', 'Topic_Perc_Contrib', 'Keywords', 'Text']\n",
        "\n",
        "# Show\n",
        "#df_dominant_topic.head(NUM_TOPICS)\n",
        "\n",
        "df_dominant_topic3.head(100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kejGWUZfVBgJ"
      },
      "source": [
        "df_dominant_topic3.head(200)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0v25BJISVBgJ"
      },
      "source": [
        "df_dominant_topic3.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XZF1xO16VBgJ"
      },
      "source": [
        "df_dominant_topic3.to_csv(Tweets_Topics_40)\n",
        "Tweets_Topics_40"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-vHppgHPVBgL"
      },
      "source": [
        "19. Find the most representative document for each topic\n",
        "Sometimes just the topic keywords may not be enough to make sense of what a topic is about. So, to help with understanding the topic, you can find the documents a given topic has contributed to the most and infer the topic by reading that document. Whew!!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P1nj3nPtVBgL"
      },
      "source": [
        "# Group top 5 sentences under each topic\n",
        "sent_topics_sorteddf_mallet3 = pd.DataFrame()\n",
        "\n",
        "sent_topics_outdf_grpd3 = df_topic_sents_keywords3.groupby('Dominant_Topic')\n",
        "print(len(df_topic_sents_keywords3))\n",
        "print(len(sent_topics_outdf_grpd3))\n",
        "\n",
        "for i, grp in sent_topics_outdf_grpd3:\n",
        "    sent_topics_sorteddf_mallet3 = pd.concat([sent_topics_sorteddf_mallet3,\n",
        "                                             grp.sort_values(['Perc_Contribution'], ascending=[0]).head(1)],\n",
        "                                            axis=0)\n",
        "\n",
        "# Reset Index\n",
        "sent_topics_sorteddf_mallet3.reset_index(drop=True, inplace=True)\n",
        "\n",
        "# Format\n",
        "sent_topics_sorteddf_mallet3.columns = ['Topic_Num', \"Topic_Perc_Contrib\", \"Keywords\", \"Text\"]\n",
        "\n",
        "df_top_doc3 = sent_topics_sorteddf_mallet3\n",
        "\n",
        "# Show\n",
        "sent_topics_sorteddf_mallet3.head(NUM_TOPICS3)\n",
        "print(sent_topics_sorteddf_mallet3.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bbveRZLLVBgM"
      },
      "source": [
        "20. Topic distribution across documents\n",
        "Finally, we want to understand the volume and distribution of topics in order to judge how widely it was discussed. The below table exposes that information."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PagyOIbuVBgM"
      },
      "source": [
        "# Number of Documents for Each Topic\n",
        "topic_counts3 = df_topic_sents_keywords3['Dominant_Topic'].value_counts()\n",
        "df_top_doc_count3 =  topic_counts3\n",
        "'''\n",
        "# Percentage of Documents for Each Topic\n",
        "topic_contribution = round(topic_counts/topic_counts.sum(), 4)\n",
        "\n",
        "# Topic Number and Keywords\n",
        "topic_num_keywords = df_topic_sents_keywords[['Dominant_Topic', 'Topic_Keywords']]\n",
        "\n",
        "# Concatenate Column wise\n",
        "df_dominant_topics = pd.concat([topic_num_keywords, topic_counts, topic_contribution], axis=1)\n",
        "\n",
        "# Change Column names\n",
        "df_dominant_topics.columns = ['Dominant_Topic', 'Topic_Keywords', 'Num_Documents', 'Perc_Documents']\n",
        "\n",
        "# Show\n",
        "df_dominant_topics\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WsxIXdWrVBgM"
      },
      "source": [
        "topic_counts3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H6OgHI8AVBgM"
      },
      "source": [
        "df_top_selected3 = df_top_doc_count3 [df_top_doc_count3 >= 0 ]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qfhPfbMFVBgN"
      },
      "source": [
        "df_top_selected3.index"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6LW6x0QVVBgO"
      },
      "source": [
        "in this step, I create a dataframe including Topic_Num, Keywords, Doc_Num"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CgmjzVq_VBgO"
      },
      "source": [
        "df_t_k_d3 = pd.DataFrame(columns = ['Topic_Num','Doc_Num','Topic', 'Keywords'])\n",
        "#df_selected.columns = ['Topic_Num','Keywords','Doc_Num']\n",
        "for i, v in df_top_selected3.iteritems():\n",
        "    wp = lda_model_3.show_topic(int(i))\n",
        "    topic_keywords = \", \".join([word for word, prop in wp])\n",
        "    new_entry = {'Topic_Num': i, 'Doc_Num': v, 'Topic': lda_model_3.show_topic(int(i)),\n",
        "                'Keywords': topic_keywords }\n",
        "    #row = [i,v]\n",
        "    df_t_k_d3.loc[len(df_t_k_d3)] = new_entry\n",
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GF0Yppw_VBgO"
      },
      "source": [
        "df_t_k_d3.to_csv(Topics_Docs_Count_40)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZBlQBvHXVBgO"
      },
      "source": [
        "df_t_k_d3.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T_B0ar579cGl"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lf82LaH49m4N"
      },
      "source": [
        "# LDA (Topic Number=80)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oxM5u4KD9m4N"
      },
      "source": [
        "NUM_TOPICS4"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Usu7Jbb9m4P"
      },
      "source": [
        "print(\"Building lda model...\")\n",
        "t0= time()\n",
        "lda_model_4 = models.ldamodel.LdaModel(corpus=corpus,\n",
        "                                    id2word=dictionary,\n",
        "                                    iterations=200,\n",
        "                                    num_topics=NUM_TOPICS4,\n",
        "                                    update_every=1,\n",
        "                                    chunksize=50,\n",
        "                                    passes=20,\n",
        "                                    alpha='auto',\n",
        "                                    per_word_topics=True)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EMkgTTit9m4P"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "## Preplexity and Coherence"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A3_spCdH9m4P"
      },
      "source": [
        "# Compute Perplexity\n",
        "print('\\nPerplexity: ', lda_model_4.log_perplexity(corpus))  # a measure of how good the model is. lower the better.\n",
        "\n",
        "# Compute Coherence Score\n",
        "coherence_model_lda_4 = CoherenceModel(model=lda_model_4, texts=tweet_content, dictionary=dictionary, coherence='c_v')\n",
        "coherence_lda_4 = coherence_model_lda_4.get_coherence()\n",
        "print('\\nCoherence Score: ', coherence_lda_4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MISjwfgV9m4P"
      },
      "source": [
        "##Finding Topics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c1PIYOom9m4P"
      },
      "source": [
        "topics4 = lda_model_4.print_topics(num_topics = NUM_TOPICS4, num_words= 15)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6I6QbcMa4sXM"
      },
      "source": [
        "len(topics4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "McLraELT0Qmr"
      },
      "source": [
        "topics4 = lda_model_4.print_topics(num_topics = NUM_TOPICS4, num_words= 15)\n",
        "df = pd.DataFrame(topics4, columns=['TopicNo', 'Topic'])\n",
        "\n",
        "keywords = []\n",
        "for i in range(0,80):\n",
        "  temp = lda_model_4.show_topic(i, 15)\n",
        "  k = []\n",
        "  for item in temp:\n",
        "    k.append(item[0])\n",
        "  keywords.append(k)\n",
        "\n",
        "\n",
        "df2 = df.assign(Keywords = keywords)\n",
        "df2.to_csv(Topics_80)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mV2UwdJy9m4P"
      },
      "source": [
        "topics4"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vfZKB0cO9m4Q"
      },
      "source": [
        "# Visualize the topics\n",
        "pyLDAvis.enable_notebook()\n",
        "vis = pyLDAvis.gensim.prepare(lda_model_4, corpus, dictionary)\n",
        "vis"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nRn7_hI49m4Q"
      },
      "source": [
        "LDA_MODEL"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WaeMwd9C9m4Q"
      },
      "source": [
        "topics4 = lda_model_4.print_topics(num_topics = NUM_TOPICS4, num_words= 15)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dADkwfyZ9m4Q"
      },
      "source": [
        "NUM_WORDS"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gODg51QJ9m4Q"
      },
      "source": [
        "topics4"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_FDGMLlI9m4R"
      },
      "source": [
        "## Saving topics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yaTGVXON9m4R"
      },
      "source": [
        "def format_topics_sentences4(ldamodel=lda_model_4, corpus=corpus, texts=tweet_content):\n",
        "    # Init output\n",
        "    sent_topics_df = pd.DataFrame()\n",
        "\n",
        "    # Get main topic in each document\n",
        "    for i, row in enumerate(ldamodel[corpus]):\n",
        "        row = sorted(row[0], key=lambda x: (x[1]), reverse=True)\n",
        "        # Get the Dominant topic, Perc Contribution and Keywords for each document\n",
        "        for j, (topic_num, prop_topic) in enumerate(row):\n",
        "            if j == 0:  # => dominant topic\n",
        "                wp = ldamodel.show_topic(topic_num)\n",
        "                topic_keywords = \", \".join([word for word, prop in wp])\n",
        "                sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n",
        "            else:\n",
        "                break\n",
        "    sent_topics_df.columns = ['Dominant_Topic', 'Perc_Contribution', 'Topic_Keywords']\n",
        "\n",
        "    # Add original text to the end of the output\n",
        "    contents = pd.Series(texts)\n",
        "    sent_topics_df = pd.concat([sent_topics_df, contents], axis=1)\n",
        "\n",
        "    return(sent_topics_df)\n",
        "\n",
        "\n",
        "df_topic_sents_keywords4 = format_topics_sentences4(ldamodel=lda_model_4, corpus=corpus, texts=tweet_content)\n",
        "\n",
        "# Format\n",
        "df_dominant_topic4 = df_topic_sents_keywords4.reset_index()\n",
        "df_dominant_topic4.columns = ['Document_No', 'Dominant_Topic', 'Topic_Perc_Contrib', 'Keywords', 'Text']\n",
        "\n",
        "# Show\n",
        "#df_dominant_topic.head(NUM_TOPICS)\n",
        "\n",
        "df_dominant_topic4.head(100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pugtMnPc9m4R"
      },
      "source": [
        "df_dominant_topic4.head(200)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ihj7s0kF9m4T"
      },
      "source": [
        "df_dominant_topic4.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fxRh48iv9m4T"
      },
      "source": [
        "df_dominant_topic4.to_csv(Tweets_Topics_80)\n",
        "Tweets_Topics_80"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PF0kikA39m4U"
      },
      "source": [
        "19. Find the most representative document for each topic\n",
        "Sometimes just the topic keywords may not be enough to make sense of what a topic is about. So, to help with understanding the topic, you can find the documents a given topic has contributed to the most and infer the topic by reading that document. Whew!!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rtrwpvhw9m4U"
      },
      "source": [
        "# Group top 5 sentences under each topic\n",
        "sent_topics_sorteddf_mallet4 = pd.DataFrame()\n",
        "\n",
        "sent_topics_outdf_grpd4 = df_topic_sents_keywords4.groupby('Dominant_Topic')\n",
        "print(len(df_topic_sents_keywords4))\n",
        "print(len(sent_topics_outdf_grpd4))\n",
        "\n",
        "for i, grp in sent_topics_outdf_grpd4:\n",
        "    sent_topics_sorteddf_mallet4 = pd.concat([sent_topics_sorteddf_mallet4,\n",
        "                                             grp.sort_values(['Perc_Contribution'], ascending=[0]).head(1)],\n",
        "                                            axis=0)\n",
        "\n",
        "# Reset Index\n",
        "sent_topics_sorteddf_mallet4.reset_index(drop=True, inplace=True)\n",
        "\n",
        "# Format\n",
        "sent_topics_sorteddf_mallet4.columns = ['Topic_Num', \"Topic_Perc_Contrib\", \"Keywords\", \"Text\"]\n",
        "\n",
        "df_top_doc4 = sent_topics_sorteddf_mallet4\n",
        "\n",
        "# Show\n",
        "sent_topics_sorteddf_mallet4.head(NUM_TOPICS4)\n",
        "print(sent_topics_sorteddf_mallet4.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V9feZEu29m4U"
      },
      "source": [
        "20. Topic distribution across documents\n",
        "Finally, we want to understand the volume and distribution of topics in order to judge how widely it was discussed. The below table exposes that information."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X-cbWuwU9m4U"
      },
      "source": [
        "# Number of Documents for Each Topic\n",
        "topic_counts4 = df_topic_sents_keywords4['Dominant_Topic'].value_counts()\n",
        "df_top_doc_count4 =  topic_counts4\n",
        "'''\n",
        "# Percentage of Documents for Each Topic\n",
        "topic_contribution = round(topic_counts/topic_counts.sum(), 4)\n",
        "\n",
        "# Topic Number and Keywords\n",
        "topic_num_keywords = df_topic_sents_keywords[['Dominant_Topic', 'Topic_Keywords']]\n",
        "\n",
        "# Concatenate Column wise\n",
        "df_dominant_topics = pd.concat([topic_num_keywords, topic_counts, topic_contribution], axis=1)\n",
        "\n",
        "# Change Column names\n",
        "df_dominant_topics.columns = ['Dominant_Topic', 'Topic_Keywords', 'Num_Documents', 'Perc_Documents']\n",
        "\n",
        "# Show\n",
        "df_dominant_topics\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FO9WeGu69m4U"
      },
      "source": [
        "topic_counts4"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EcRcrw039m4U"
      },
      "source": [
        "df_top_selected4 = df_top_doc_count4 [df_top_doc_count4 >= 0 ]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k_84AYuH9m4U"
      },
      "source": [
        "df_top_selected4.index"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "voBU8tm79m4U"
      },
      "source": [
        "in this step, I create a dataframe including Topic_Num, Keywords, Doc_Num"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pg_V9uDT9m4U"
      },
      "source": [
        "df_t_k_d4 = pd.DataFrame(columns = ['Topic_Num','Doc_Num','Topic', 'Keywords'])\n",
        "#df_selected.columns = ['Topic_Num','Keywords','Doc_Num']\n",
        "for i, v in df_top_selected4.iteritems():\n",
        "    wp = lda_model_4.show_topic(int(i))\n",
        "    topic_keywords = \", \".join([word for word, prop in wp])\n",
        "    new_entry = {'Topic_Num': i, 'Doc_Num': v, 'Topic': lda_model_4.show_topic(int(i)),\n",
        "                'Keywords': topic_keywords }\n",
        "    #row = [i,v]\n",
        "    df_t_k_d4.loc[len(df_t_k_d4)] = new_entry\n",
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kib5WrLQ9m4U"
      },
      "source": [
        "df_t_k_d4.to_csv(Topics_Docs_Count_80)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1sWymkye9m4U"
      },
      "source": [
        "df_t_k_d4.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ENVRSsFG9m4V"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fRgPrzwx9m4V"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}