{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "xH5kk5CeRNy1",
        "fP1soi0P7w4B",
        "S_Gh5cAy7-3S",
        "0Og_e-oZ81d0",
        "3t0lR8HC_QCN",
        "QGEGeZppj-dk",
        "Z6jisWhaVR9A",
        "9mlePpnIt0rm",
        "u3vqpI5_4aaH",
        "byUw4FUoJeWg",
        "W8v3DRX0MPJF",
        "6gqt70t799a9",
        "uPTewWYOGtT0",
        "eX1hY2cdGwv3",
        "QZXeaWWaIPnQ",
        "Beq11JiYJB54",
        "-CQaBfqPeVZN"
      ],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/seilsepour/SSTSA/blob/main/OverAll_Deep.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yGhQTu6q5upq"
      },
      "source": [
        "\n",
        "How to calculate metrics\n",
        "https://datascience.stackexchange.com/questions/45165/how-to-get-accuracy-f1-precision-and-recall-for-a-keras-model\n",
        "\n",
        "\n",
        "https://stackabuse.com/python-for-nlp-movie-sentiment-analysis-using-deep-learning-in-keras/\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3d1QKLFp6g9v"
      },
      "source": [
        "Increasing the number of layers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gq8vE27pDj7n"
      },
      "source": [
        "ÙŒWord2vec Dimension = 500"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%tensorflow_version 1.x"
      ],
      "metadata": {
        "id": "kQFCEakG5P-h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow\n",
        "print(tensorflow.__version__)"
      ],
      "metadata": {
        "id": "YYGcpWNn5U5_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_BwYl-bpgEdE"
      },
      "source": [
        "\n",
        "topicNumber = 70"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RSRYixW1yEsL"
      },
      "source": [
        "#pip install keras --user"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e3d7jwGaa-Y9"
      },
      "source": [
        "# Configurations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4w1SUxFB1jAx"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import string\n",
        "from __future__ import print_function\n",
        "\n",
        "import keras\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras.preprocessing import sequence\n",
        "from keras.models import Sequential\n",
        "from keras.layers import SimpleRNN, Dense, Dropout, Embedding, LSTM, GRU, Bidirectional, Activation, Conv1D, MaxPooling1D, GlobalMaxPooling1D, BatchNormalization\n",
        "#from keras.optimizers import SGD\n",
        "from keras.layers import SpatialDropout1D, Flatten\n",
        "from keras.callbacks import Callback\n",
        "from keras.callbacks import ReduceLROnPlateau\n",
        "from keras import backend\n",
        "from keras.initializers import Constant\n",
        "from keras.regularizers import l2\n",
        "from sklearn.metrics import precision_recall_fscore_support as score\n",
        "\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "from matplotlib.pyplot import plot\n",
        "\n",
        "\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "allTrainingData = '/MYDRIVE/My Drive/Colab Notebooks/SecondPaper/data/70-with-labels.xlsx'"
      ],
      "metadata": {
        "id": "IiRnrThH-2ty"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mve9CxfcQ4Tf"
      },
      "source": [
        "w2vMovieTxt = '/MYDRIVE/My Drive/Colab Notebooks/Proposal/Data/w2vMovie-MR-99-07-25.txt'\n",
        "w2vMovieM = '/MYDRIVE/My Drive/Colab Notebooks/Proposal/Data/w2vMovie-MR-99-07-25.h5'\n",
        "\n",
        "lstmModelTxt = '/MYDRIVE/My Drive/Colab Notebooks/Proposal/Data/lstm-99-07-25.txt'\n",
        "lstmModelH = '/MYDRIVE/My Drive/Colab Notebooks/Proposal/Data/lstm-99-07-25.h5'\n",
        "\n",
        "gruModelTxt = '/MYDRIVE/My Drive/Colab Notebooks/Proposal/Data/gru-99-07-25.txt'\n",
        "gruModelH = '/MYDRIVE/My Drive/Colab Notebooks/Proposal/Data/gru-99-07-25.h5'\n",
        "\n",
        "bilstmModelTxt = '/MYDRIVE/My Drive/Colab Notebooks/Proposal/Data/bilstm-99-07-25.txt'\n",
        "bilstmModelH = '/MYDRIVE/My Drive/Colab Notebooks/Proposal/Data/bilstm-99-07-25.h5'\n",
        "\n",
        "conv1ModelTxt = '/MYDRIVE/My Drive/Colab Notebooks/Proposal/Data/conv1-99-07-25.txt'\n",
        "conv1ModelH = '/MYDRIVE/My Drive/Colab Notebooks/Proposal/Data/conv1-99-07-25.h5'\n",
        "\n",
        "CLModelTxt = '/MYDRIVE/My Drive/Colab Notebooks/Proposal/Data/Cl-99-07-25.txt'\n",
        "CLModelH = '/MYDRIVE/My Drive/Colab Notebooks/Proposal/Data/Cl-99-07-25.h5'\n",
        "\n",
        "#This file includes tweets and topic number\n",
        "tweetsTopics = '/MYDRIVE/My Drive/Colab Notebooks/Proposal/Data/Tweets_Topics_990725.csv'\n",
        "\n",
        "\n",
        "mr8Similar = '/MYDRIVE/My Drive/Colab Notebooks/Proposal/Data/SimilarDocsByProposedMethod/Topics_8_MostSimilar_PM.csv'\n",
        "\n",
        "\n",
        "\n",
        "pangLabelledDataset = '/MYDRIVE/My Drive/Colab Notebooks/Proposal/Data/MovieReview-Preprocessed-Pang-Ver2-Labelled.csv'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xH5kk5CeRNy1"
      },
      "source": [
        "##Hyper parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aD5t-DgbRRjp"
      },
      "source": [
        "#Making Word2vec\n",
        "embeddingDim = 500\n",
        "maxLen = embeddingDim\n",
        "testSize=0.2\n",
        "windowSize=5\n",
        "minCount=1\n",
        "iterCount=10\n",
        "sG = 0 #skip gram wil be used\n",
        "\n",
        "\n",
        "#Making Model\n",
        "validationSplit = 0.2\n",
        "#maxFeatures = numWords\n",
        "maxLen = 500\n",
        "embeddingSize = 500\n",
        "isTrainable = False\n",
        "batchSize = 16\n",
        "\n",
        "\n",
        "\n",
        "epochsNum = 20\n",
        "\n",
        "unitsNum = 20\n",
        "\n",
        "\n",
        "# Convolution\n",
        "kernelSize = 4\n",
        "\n",
        "filtersNum = 32\n",
        "\n",
        "\n",
        "poolSize = 2\n",
        "stridesNum = 1\n",
        "hidden_dims = 100\n",
        "\n",
        "# LSTM\n",
        "lstm_output_size = 70"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vZfpicuARSxj"
      },
      "source": [
        "#Login to Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wiL9CUU_RICQ"
      },
      "source": [
        "#No need in local\n",
        "from google.colab import drive\n",
        "drive.mount('/MYDRIVE', force_remount=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zk2chpFWcue0"
      },
      "source": [
        "# Sentiment Analysis by W2V Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fP1soi0P7w4B"
      },
      "source": [
        "## Loading all Training labelled data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UONqNxvkOttf"
      },
      "source": [
        "allTrainingData"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dfALLTraining = pd.read_excel(allTrainingData)\n",
        "print(dfALLTraining.columns)"
      ],
      "metadata": {
        "id": "DCrrYsm5_Kpz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dfALLTraining = dfALLTraining[['id','PredictedLabel','ActualText']]\n",
        "dfALLTraining.columns = [\"docid\",\"Sentiment\",\"Text\"]\n"
      ],
      "metadata": {
        "id": "77FMvuPM_gxD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xmGV_OVuMM1F"
      },
      "source": [
        "print(dfALLTraining.head(3))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vRznz_3VTDZS"
      },
      "source": [
        "print(len(dfALLTraining[dfALLTraining[\"Sentiment\"] == 0]))\n",
        "print(len(dfALLTraining[dfALLTraining['Sentiment'] == 1]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S_Gh5cAy7-3S"
      },
      "source": [
        "## Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7mCRpHWLa_4h"
      },
      "source": [
        "dfALLTraining.head(3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from string import punctuation\n",
        "from os import listdir\n",
        "from nltk.corpus import stopwords\n",
        "from pickle import dump\n",
        "\n",
        "def clean_doc(doc):\n",
        "\t# split into tokens by white space\n",
        "\ttokens = doc.split()\n",
        "\t# remove punctuation from each token\n",
        "\ttable = str.maketrans('', '', punctuation)\n",
        "\ttokens = [w.translate(table) for w in tokens]\n",
        "\t# remove remaining tokens that are not alphabetic\n",
        "\ttokens = [word for word in tokens if word.isalpha()]\n",
        "\t# filter out stop words\n",
        "\tstop_words = set(stopwords.words('english'))\n",
        "\ttokens = [w for w in tokens if not w in stop_words]\n",
        "\t# filter out short tokens\n",
        "\ttokens = [word for word in tokens if len(word) > 1]\n",
        "\ttokens = ' '.join(tokens)\n",
        "\treturn tokens"
      ],
      "metadata": {
        "id": "7pJJ8RlS8UnE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for index, row in dfALLTraining.iterrows():\n",
        "     dfALLTraining.at[index,'Text']= clean_doc(row['Text'])\n"
      ],
      "metadata": {
        "id": "7z4Xu0xKBNvE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dfALLTraining.head(3)"
      ],
      "metadata": {
        "id": "LC6IRmKkCES5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tYom0c3lmklO"
      },
      "source": [
        "print(dfALLTraining.nunique())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1aGuRrwXldk0"
      },
      "source": [
        "print(max(dfALLTraining['docid']))\n",
        "#print(max(dfALLTraining['index']))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vq5z-WtzaB-P"
      },
      "source": [
        "dfTrainingMovies = dfALLTraining"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NRU8E9owaWFs"
      },
      "source": [
        "numNegs = len(dfTrainingMovies[dfTrainingMovies['Sentiment'] == 0])\n",
        "print(numNegs)\n",
        "numNegs = len(dfTrainingMovies[dfTrainingMovies['Sentiment'] == 1])\n",
        "print(numNegs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dfTrainingMovies.columns"
      ],
      "metadata": {
        "id": "kBq3ktk5GShw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Og_e-oZ81d0"
      },
      "source": [
        "## Making Word2Vec Model of movies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y2fimfOr824U"
      },
      "source": [
        "#embeddingDim = 100\n",
        "#Remove repeated rows\n",
        "print(dfTrainingMovies.head())\n",
        "dfMoviesNoRep = dfTrainingMovies\n",
        "#dfMoviesNoRep.columns = ['i','TweetId', 'Tweet', 'Sentiment']\n",
        "print(dfMoviesNoRep.head(3))\n",
        "#dfMoviesNoRep.sort_values([\"TweetId\"], inplace = True)\n",
        "print(dfMoviesNoRep.head(3))\n",
        "# dropping ALL duplicte values\n",
        "#dfMoviesNoRep.drop_duplicates(subset =\"TweetId\",\n",
        " #                    keep = 'first', inplace = True)\n",
        "print(len(dfMoviesNoRep))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qa2dDtwBAQXu"
      },
      "source": [
        "numNegs = len(dfMoviesNoRep[dfMoviesNoRep['Sentiment'] == 0])\n",
        "print(numNegs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DGxjHGYwAqs-"
      },
      "source": [
        "numPoss = len(dfMoviesNoRep[dfMoviesNoRep['Sentiment'] == 1])\n",
        "print(numPoss)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "diff = numNegs - numPoss"
      ],
      "metadata": {
        "id": "OF_2s_SDHq0L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IjSZ9vj_P9RK"
      },
      "source": [
        "# Making a balance dataset with equal number of positive and negative tweets\n",
        "df1 = dfMoviesNoRep[dfMoviesNoRep['Sentiment'] == 1].sample(diff)\n",
        "#df2 = dfMoviesNoRep[dfMoviesNoRep['Sentiment'] == 0].sample(numPoss)\n",
        "df = dfMoviesNoRep.append(df1)\n",
        "print(len(df[df['Sentiment'] == 1]))\n",
        "print(len(df[df['Sentiment'] == 0]))\n",
        "print(df.head())\n",
        "print(len(df))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GgFEkJXA6Ouh"
      },
      "source": [
        "Analyzing Probability"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7AkcksOm9GrL"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "x_train_m, x_test_m, y_train_m, y_test_m = train_test_split( df['Text'], df['Sentiment'], test_size=0.2, random_state=10)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train_m"
      ],
      "metadata": {
        "id": "7O05TpfMu1iZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qao0W_pL9R7l"
      },
      "source": [
        "print(len(x_test_m))\n",
        "print(len(x_train_m))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ENuvrNGaD0b1"
      },
      "source": [
        "print(len(y_test_m[y_test_m == 0]))\n",
        "print(len(y_test_m[y_test_m == 1]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QN6GDBRMmED1"
      },
      "source": [
        "print(len(y_train_m[y_train_m == 0]))\n",
        "print(len(y_train_m[y_train_m == 1]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zxjq4l2XhdZD"
      },
      "source": [
        "reviewMovieLines = list()\n",
        "for line in x_train_m:\n",
        "  reviewMovieLines.append( str(line).split() )\n",
        "\n",
        "for line in x_test_m:\n",
        "  reviewMovieLines.append( str(line).split() )\n",
        "\n",
        "print(len(reviewMovieLines))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PAYD3fv99i_X"
      },
      "source": [
        "import gensim\n",
        "\n",
        "\n",
        "w2vMovie = gensim.models.Word2Vec( sentences=reviewMovieLines, size=embeddingDim,  window=windowSize, workers=4, min_count=minCount, iter=iterCount, sg = sG )\n",
        "\n",
        "w2vGensimModel = gensim.models.Word2Vec()\n",
        "#vocab size\n",
        "\n",
        "words = list( w2vMovie.wv.vocab )\n",
        "\n",
        "print('Vocabulary size: %d' % len(words))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zKy7YUy504rl"
      },
      "source": [
        "w2vMovieTxt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YAQlz8Rk-nUU"
      },
      "source": [
        "w2vMovie.wv.save_word2vec_format( w2vMovieTxt, binary = False)\n",
        "w2vMovie.save(w2vMovieM)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6_jkEURK-1tX"
      },
      "source": [
        "w2vMovie.wv.most_similar('film')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3t0lR8HC_QCN"
      },
      "source": [
        "# Desining new network by w2v"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "91tbeVAsc243"
      },
      "source": [
        "import os\n",
        "embeddingsIndex = {}\n",
        "f = open(os.path.join('', w2vMovieTxt ), encoding = \"utf-8\")\n",
        "\n",
        "for line in f:\n",
        "  values = line.split()\n",
        "  word = values[0]\n",
        "  coefs = np.asarray(values[1:])\n",
        "  embeddingsIndex[word] = coefs\n",
        "f.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JO9Rk-6CdYrX"
      },
      "source": [
        "tokenizerObj = Tokenizer()\n",
        "tokenizerObj.fit_on_texts(reviewMovieLines)\n",
        "sequences = tokenizerObj.texts_to_sequences(reviewMovieLines)\n",
        "\n",
        "print(type(sequences))\n",
        "\n",
        "reviewPad = pad_sequences( sequences, maxlen=maxLen)\n",
        "\n",
        "wordIndex = tokenizerObj.word_index\n",
        "print('Found %s unique tokens.' % len(wordIndex))\n",
        "\n",
        "#reviewPad = pad_sequences( sequences, maxlen=maxLen)\n",
        "#sentiment = df ['Sentiment'].values\n",
        "print(type(reviewPad))\n",
        "print(len(reviewPad))\n",
        "\n",
        "x_train_pad = reviewPad[ : len(x_train_m)]\n",
        "x_test_pad = reviewPad[ len(x_train_m) : ]\n",
        "y_train = y_train_m\n",
        "y_test = y_test_m\n",
        "\n",
        "print(type(x_train_pad))\n",
        "\n",
        "print('Shape of x_train_pad tensor:' , len(x_train_pad))\n",
        "print('Shape of y_train tensor:' , len(y_train))\n",
        "\n",
        "\n",
        "print('Shape of x_test_pad tensor:' , len(x_test_pad))\n",
        "print('Shape of y_test tensor:' , len( y_test ))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WKiYo7J-tX4j"
      },
      "source": [
        "x = y_test[y_test == 0]\n",
        "print(len(x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Acb97fyUDeTn"
      },
      "source": [
        "x = y_test[y_test == 1]\n",
        "print(len(x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RiCiwBFGf8r5"
      },
      "source": [
        "x = y_train[y_train == 0]\n",
        "print(len(x))\n",
        "x = y_train[y_train == 1]\n",
        "print(len(x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XLSV3pFcFEE3"
      },
      "source": [
        "embeddingDim"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "79qVnwskddTl"
      },
      "source": [
        "numWords = len(wordIndex) + 1\n",
        "print(numWords)\n",
        "embeddingMatrix = np.zeros( (numWords, embeddingDim) )\n",
        "#print(embeddingMatrix.shape)\n",
        "for word, i in wordIndex.items():\n",
        "  if i > numWords:\n",
        "    continue\n",
        "  embeddingVector = embeddingsIndex.get(word)\n",
        "  if embeddingVector is not None:\n",
        "    # words not found in embedding index will be all-zeros.\n",
        "    embeddingMatrix[i] = embeddingVector"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MXzYzYzmlbX9"
      },
      "source": [
        "print(embeddingMatrix.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fvc_NhFlp15v"
      },
      "source": [
        "max_features = numWords"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "21gDmcUUGOw2"
      },
      "source": [
        "\n",
        "# CNN+GRU\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5CORclx3GPMK"
      },
      "source": [
        "import tensorflow as tf\n",
        "epochsNum = 20\n",
        "batchSize = 16\n",
        "\n",
        "filtersNum = 32\n",
        "\n",
        "max_features = numWords\n",
        "\n",
        "# Convolution\n",
        "\n",
        "print('Build model...')\n",
        "\n",
        "modelCGRU = Sequential(name='CNNGRU')\n",
        "\n",
        "modelCGRU.add(Embedding(numWords, embeddingDim ,\n",
        "                      embeddings_initializer = Constant( embeddingMatrix),\n",
        "                      input_length=maxLen,\n",
        "                      trainable = isTrainable))\n",
        "\n",
        "\n",
        "modelCGRU.add(Conv1D(filters= filtersNum, kernel_size= 4, strides= stridesNum, padding='same', activation='relu'))\n",
        "modelCGRU.add(Dropout(0.1))\n",
        "\n",
        "modelCGRU.add(Conv1D(filters= filtersNum, kernel_size= 4, strides= stridesNum, padding='same', activation='relu'))\n",
        "modelCGRU.add(Dropout(0.1))\n",
        "\n",
        "modelCGRU.add(MaxPooling1D(pool_size = 2))\n",
        "\n",
        "modelCGRU.add(Dropout(0.1))\n",
        "modelCGRU.add(GRU(10, dropout=0.1 ))\n",
        "modelCGRU.add(Dropout(0.1))\n",
        "\n",
        "modelCGRU.add(Dense(5, activation='relu'))\n",
        "\n",
        "modelCGRU.add(Dense(1,activation='sigmoid'))\n",
        "\n",
        "\n",
        "\n",
        "modelCGRU.compile(loss='binary_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])\n",
        "print('Train...')\n",
        "\n",
        "historyCGRU = modelCGRU.fit(x_train_pad, y_train,\n",
        "          batch_size=batchSize,\n",
        "          epochs=epochsNum,\n",
        "          validation_data=(x_test_pad, y_test), verbose = 2 )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dE2II_wmsKhX"
      },
      "source": [
        "testLossCGRU, testAccCGRU = modelCGRU.evaluate(x_test_pad, y_test, batch_size=batchSize, verbose=1)\n",
        "print('Test loss:', testLossCGRU)\n",
        "print('Test accuracy:', testAccCGRU)\n",
        "\n",
        "trainLossCGRU, trainAccCGRU = modelCGRU.evaluate(x_train_pad, y_train, batch_size=batchSize, verbose=1)\n",
        "print('Train loss:', trainLossCGRU)\n",
        "print('Train accuracy:', trainAccCGRU)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "URzFMln7IRx4"
      },
      "source": [
        "plt.title('Accuracy')\n",
        "plt.plot(historyCGRU.history['accuracy'], label='train')\n",
        "plt.plot(historyCGRU.history['val_accuracy'], label='test')\n",
        "plt.legend()\n",
        "plt.show();"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z6lDpaOGITgj"
      },
      "source": [
        "plt.title('Loss')\n",
        "plt.plot(historyCGRU.history['loss'], label='train')\n",
        "plt.plot(historyCGRU.history['val_loss'], label='test')\n",
        "plt.legend()\n",
        "plt.show();"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow\n",
        "print(tensorflow.__version__)"
      ],
      "metadata": {
        "id": "aJeCXufq41bF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_val_pred_cg=modelCGRU.predict_classes(x_test_pad)\n",
        "#y_val_pred_cg = modelCGRU.predict(x_test_pad)\n",
        "precisionCGRU, recallCGRU, fscoreCGRU, supportCGRU = score(y_test, y_val_pred_cg)\n",
        "\n",
        "print('precision: {}'.format(precisionCGRU))\n",
        "print('recall: {}'.format(recallCGRU))\n",
        "print('fscore: {}'.format(fscoreCGRU))\n",
        "print('support: {}'.format(supportCGRU))"
      ],
      "metadata": {
        "id": "pSKXn99fKXP_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EF4zLSw-8vuj"
      },
      "source": [
        "from sklearn.metrics import cohen_kappa_score\n",
        "\n",
        "cohen_score = cohen_kappa_score(y_test, y_val_pred_cg)\n",
        "\n",
        "print(cohen_score)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5SUSzvgM6BUm"
      },
      "source": [
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix\n",
        "labels = [0,1]\n",
        "def cm_analysis(y_true, y_pred, labels, ymap=None, figsize=(10,10)):\n",
        "    \"\"\"\n",
        "    Generate matrix plot of confusion matrix with pretty annotations.\n",
        "    The plot image is saved to disk.\n",
        "    args:\n",
        "      y_true:    true label of the data, with shape (nsamples,)\n",
        "      y_pred:    prediction of the data, with shape (nsamples,)\n",
        "      filename:  filename of figure file to save\n",
        "      labels:    string array, name the order of class labels in the confusion matrix.\n",
        "                 use `clf.classes_` if using scikit-learn models.\n",
        "                 with shape (nclass,).\n",
        "      ymap:      dict: any -> string, length == nclass.\n",
        "                 if not None, map the labels & ys to more understandable strings.\n",
        "                 Caution: original y_true, y_pred and labels must align.\n",
        "      figsize:   the size of the figure plotted.\n",
        "    \"\"\"\n",
        "    if ymap is not None:\n",
        "        y_pred = [ymap[yi] for yi in y_pred]\n",
        "        y_true = [ymap[yi] for yi in y_true]\n",
        "        labels = [ymap[yi] for yi in labels]\n",
        "    cm = confusion_matrix(y_true, y_pred, labels=labels)\n",
        "    cm_sum = np.sum(cm, axis=1, keepdims=True)\n",
        "    cm_perc = cm / cm_sum.astype(float) * 100\n",
        "    annot = np.empty_like(cm).astype(str)\n",
        "    nrows, ncols = cm.shape\n",
        "    for i in range(nrows):\n",
        "        for j in range(ncols):\n",
        "            c = cm[i, j]\n",
        "            p = cm_perc[i, j]\n",
        "            if i == j:\n",
        "                s = cm_sum[i]\n",
        "                annot[i, j] = '%.1f%%\\n%d/%d' % (p, c, s)\n",
        "            elif c == 0:\n",
        "                annot[i, j] = ''\n",
        "            else:\n",
        "                annot[i, j] = '%.1f%%\\n%d' % (p, c)\n",
        "    cm = pd.DataFrame(cm, index=labels, columns=labels)\n",
        "    cm.index.name = 'Actual'\n",
        "    cm.columns.name = 'Predicted'\n",
        "    #fig, ax = plt.subplots(figsize=figsize)\n",
        "    fig, ax = plt.subplots()\n",
        "    sns.heatmap(cm, annot=annot, fmt='', ax=ax)\n",
        "    #plt.savefig(filename)\n",
        "    plt.show()\n",
        "\n",
        "cm_analysis(y_test, y_val_pred_cg, labels, ymap=None, figsize=(5,5))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QGEGeZppj-dk"
      },
      "source": [
        "# CNN+BIGRU\n",
        "sgd"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fV73BuEej-dm"
      },
      "source": [
        "from keras.layers import GlobalMaxPooling1D\n",
        "\n",
        "max_features = numWords\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Convolution\n",
        "\n",
        "print('Build model...')\n",
        "\n",
        "modelCBIGRU = Sequential(name='CNNBIGRU')\n",
        "\n",
        "modelCBIGRU.add(Embedding(numWords, embeddingDim ,\n",
        "                      embeddings_initializer = Constant( embeddingMatrix),\n",
        "                      input_length=maxLen,\n",
        "                      trainable = isTrainable))\n",
        "\n",
        "#modelCGRU.add(Dropout(0.5))\n",
        "\n",
        "modelCBIGRU.add(Conv1D(filters= filtersNum, kernel_size= 4, strides= stridesNum, padding='same', activation='relu'))\n",
        "\n",
        "\n",
        "modelCBIGRU.add(MaxPooling1D(pool_size = 2))\n",
        "modelCBIGRU.add(Dropout(0.1))\n",
        "modelCBIGRU.add(Bidirectional(GRU(10, dropout=0.1 )))\n",
        "modelCBIGRU.add(BatchNormalization())\n",
        "modelCBIGRU.add(Dropout(0.5))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#modelCGRU_s.add(Dropout(0.6))\n",
        "\n",
        "'''\n",
        "Good for big dataset\n",
        "modelCGRU_s.add(Conv1D(filters= filtersNum, kernel_size= 3, strides= stridesNum, padding='same', activation='relu'))\n",
        "modelCGRU_s.add(Dropout(0.6))\n",
        "modelCGRU_s.add(MaxPooling1D(pool_size=2))\n",
        "modelCGRU_s.add(LSTM(32))\n",
        "modelCGRU_s.add(Dropout(0.6))\n",
        "'''\n",
        "modelCBIGRU.add(Dense(1,activation='sigmoid'))\n",
        "\n",
        "modelCBIGRU.compile(loss='binary_crossentropy',\n",
        "              optimizer= 'adam',\n",
        "              metrics=['accuracy'])\n",
        "print('Train...')\n",
        "rlrp = ReduceLROnPlateau(monitor='val_loss',\n",
        "                                  factor=0.2,\n",
        "                                  patience=4,\n",
        "                                  verbose=1,\n",
        "                                  mode='auto',\n",
        "                                  min_delta=0.0001,\n",
        "                                  cooldown=8,\n",
        "                                  min_lr=0)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NV_50YrYWsHs"
      },
      "source": [
        "historyCBIGRU = modelCBIGRU.fit(x_train_pad, y_train,\n",
        "          batch_size=batchSize,\n",
        "          epochs=epochsNum,\n",
        "          validation_data=(x_test_pad, y_test), verbose = 2 )#, callbacks=[rlrp])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E2UNHPRUTF0d"
      },
      "source": [
        "modelCBIGRU.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1kCtamTXj-du"
      },
      "source": [
        "testLossCBIGRU, testAccCBIGRU = modelCBIGRU.evaluate(x_test_pad, y_test, batch_size=batchSize, verbose=1)\n",
        "print('Test loss:', testLossCBIGRU)\n",
        "print('Test accuracy:', testAccCBIGRU)\n",
        "\n",
        "trainLossCBIGRU, trainAccCBIGRU = modelCBIGRU.evaluate(x_train_pad, y_train, batch_size=batchSize, verbose=1)\n",
        "print('Train loss:', trainLossCBIGRU)\n",
        "print('Train accuracy:', trainAccCBIGRU)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dPMxZphEj-dx"
      },
      "source": [
        "modelCBIGRU.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SIHGj1IPj-d3"
      },
      "source": [
        "plt.title('Accuracy')\n",
        "plt.plot(historyCBIGRU.history['accuracy'], label='train')\n",
        "plt.plot(historyCBIGRU.history['val_accuracy'], label='test')\n",
        "plt.legend()\n",
        "plt.show();"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dKsvFV9sj-eB"
      },
      "source": [
        "plt.title('Loss')\n",
        "plt.plot(historyCBIGRU.history['loss'], label='train')\n",
        "plt.plot(historyCBIGRU.history['val_loss'], label='test')\n",
        "plt.legend()\n",
        "plt.show();"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hWndSlKqj-eF"
      },
      "source": [
        "y_val_pred_cbgru=modelCBIGRU.predict_classes(x_test_pad)\n",
        "precisionCBIGRU, recallCBIGRU, fscoreCBIGRU, supportCBIGRU = score(y_test, y_val_pred_cbgru)\n",
        "\n",
        "print('precision: {}'.format(precisionCBIGRU))\n",
        "print('recall: {}'.format(recallCBIGRU))\n",
        "print('fscore: {}'.format(fscoreCBIGRU))\n",
        "print('support: {}'.format(supportCBIGRU))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "34KOCnr8JyqX"
      },
      "source": [
        "from sklearn.metrics import cohen_kappa_score\n",
        "cohen_score = cohen_kappa_score(y_test, y_val_pred_cbgru)\n",
        "print(cohen_score)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XJeGkVk31y1q"
      },
      "source": [
        "len(y_test[y_test==1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z6jisWhaVR9A"
      },
      "source": [
        "# CNN+BiLSTM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5phUyoo9VWxu"
      },
      "source": [
        "# Embedding\n",
        "\n",
        "max_features = numWords\n",
        "\n",
        "\n",
        "# Convolution\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "print('Build model...')\n",
        "\n",
        "modelCBiLSTM = Sequential(name='CNNBiLSTM')\n",
        "modelCBiLSTM.add(Embedding(numWords, embeddingDim ,\n",
        "                      embeddings_initializer = Constant( embeddingMatrix),\n",
        "                      input_length=maxLen,\n",
        "                      trainable = isTrainable))\n",
        "\n",
        "\n",
        "modelCBiLSTM.add(Conv1D(filters= filtersNum, kernel_size= 4, strides= stridesNum, padding='same', activation='relu'))\n",
        "\n",
        "\n",
        "modelCBiLSTM.add(Dropout(0.1))\n",
        "modelCBiLSTM.add(MaxPooling1D(pool_size = 2))\n",
        "\n",
        "modelCBiLSTM.add(Dropout(0.1))\n",
        "modelCBiLSTM.add(Bidirectional(LSTM(10, dropout=0.1 )))\n",
        "modelCBiLSTM.add(BatchNormalization())\n",
        "modelCBiLSTM.add(Dropout(0.1))\n",
        "\n",
        "modelCBiLSTM.add(Dense(1,activation='sigmoid'))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "modelCBiLSTM.compile(loss='binary_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "\n",
        "#modelCBiLSTM.summary()\n",
        "\n",
        "print('Train...')\n",
        "\n",
        "rlrp = ReduceLROnPlateau(monitor='val_loss',\n",
        "                                  factor=0.1,\n",
        "                                  patience=8,\n",
        "                                  verbose=1,\n",
        "                                  mode='auto',\n",
        "                                  min_delta=0.0001,\n",
        "                                  cooldown=8,\n",
        "                                  min_lr=0)\n",
        "#lrm = LearningRateMonitor()\n",
        "\n",
        "historyCBiLSTM = modelCBiLSTM.fit(x_train_pad, y_train,\n",
        "          batch_size=batchSize,\n",
        "          epochs=epochsNum,\n",
        "          validation_data=(x_test_pad, y_test), verbose = 2 )#, callbacks=[rlrp])\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qIhlM7OZV7Rg"
      },
      "source": [
        "testLossCBiLSTM, testAccCBiLSTM = modelCBiLSTM.evaluate(x_test_pad, y_test, batch_size=batchSize, verbose=1)\n",
        "print('Test loss:', testLossCBiLSTM)\n",
        "print('Test accuracy:', testAccCBiLSTM)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "azSiqhM1WFCc"
      },
      "source": [
        "trainLossCBiLSTM, trainAccCBiLSTM = modelCBiLSTM.evaluate(x_train_pad, y_train, batch_size=batchSize, verbose=1)\n",
        "print('Train loss:', trainLossCBiLSTM)\n",
        "print('Train accuracy:', trainAccCBiLSTM)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hwFdy-bfWMTt"
      },
      "source": [
        "modelCBiLSTM.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y5D13bBIWO7G"
      },
      "source": [
        "plt.title('Accuracy')\n",
        "plt.plot(historyCBiLSTM.history['accuracy'], label='train')\n",
        "plt.plot(historyCBiLSTM.history['val_accuracy'], label='test')\n",
        "plt.legend()\n",
        "plt.show();"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ucEBpXrkWRL8"
      },
      "source": [
        "plt.title('Loss')\n",
        "plt.plot(historyCBiLSTM.history['loss'], label='train')\n",
        "plt.plot(historyCBiLSTM.history['val_loss'], label='test')\n",
        "plt.legend()\n",
        "plt.show();"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EtRbWLCpWW0n"
      },
      "source": [
        "y_val_pred_cbl = modelCBiLSTM.predict_classes(x_test_pad)\n",
        "precisionCBiLSTM, recallCBiLSTM, fscoreCBiLSTM, supportCBiLSTM = score(y_test, y_val_pred_cbl)\n",
        "\n",
        "print('precision: {}'.format(precisionCBiLSTM))\n",
        "print('recall: {}'.format(recallCBiLSTM))\n",
        "print('fscore: {}'.format(fscoreCBiLSTM))\n",
        "print('support: {}'.format(supportCBiLSTM))\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bQ8t2Qpd_tOp"
      },
      "source": [
        "from sklearn.metrics import cohen_kappa_score\n",
        "\n",
        "cohen_score = cohen_kappa_score(y_test, y_val_pred_cbl)\n",
        "\n",
        "print(cohen_score)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9mlePpnIt0rm"
      },
      "source": [
        "# CNN+LSTM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LFj9y14WezRQ"
      },
      "source": [
        "# Embedding\n",
        "\n",
        "\n",
        "\n",
        "max_features = numWords\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Convolution\n",
        "\n",
        "\n",
        "\n",
        "'''\n",
        "Note:\n",
        "batch_size is highly sensitive.\n",
        "Only 2 epochs are needed as the dataset is very small.\n",
        "'''\n",
        "print('Build model...')\n",
        "\n",
        "modelCL = Sequential(name='CNNLSTM')\n",
        "modelCL.add(Embedding(numWords, embeddingDim ,\n",
        "                      embeddings_initializer = Constant( embeddingMatrix),\n",
        "                      input_length=maxLen,\n",
        "                      trainable = isTrainable))\n",
        "\n",
        "\n",
        "modelCL.add(Conv1D(filters= filtersNum, kernel_size= 4, strides= stridesNum, padding='same', activation='relu'))\n",
        "\n",
        "\n",
        "modelCL.add(Dropout(0.1))\n",
        "modelCL.add(MaxPooling1D(pool_size = 2))\n",
        "\n",
        "modelCL.add(Dropout(0.1))\n",
        "modelCL.add(LSTM(10, dropout=0.1 ))\n",
        "modelCL.add(BatchNormalization())\n",
        "modelCL.add(Dropout(0.1))\n",
        "\n",
        "\n",
        "\n",
        "modelCL.add(Dense(1,activation='sigmoid'))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "modelCL.compile(loss='binary_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "\n",
        "#modelCL.summary()\n",
        "\n",
        "print('Train...')\n",
        "\n",
        "rlrp = ReduceLROnPlateau(monitor='val_loss',\n",
        "                                  factor=0.1,\n",
        "                                  patience=8,\n",
        "                                  verbose=1,\n",
        "                                  mode='auto',\n",
        "                                  min_delta=0.0001,\n",
        "                                  cooldown=8,\n",
        "                                  min_lr=0)\n",
        "#lrm = LearningRateMonitor()\n",
        "\n",
        "historyCL = modelCL.fit(x_train_pad, y_train,\n",
        "          batch_size=batchSize,\n",
        "          epochs=epochsNum,\n",
        "          validation_data=(x_test_pad, y_test), verbose = 2 )#, callbacks=[rlrp])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qIQjanOVrRpC"
      },
      "source": [
        "testLossCL, testAccCL = modelCL.evaluate(x_test_pad, y_test, batch_size=batchSize, verbose=1)\n",
        "print('Test loss:', testLossCL)\n",
        "print('Test accuracy:', testAccCL)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oS9ZA_i_22Ap"
      },
      "source": [
        "trainLossCL, trainAccCL = modelCL.evaluate(x_train_pad, y_train, batch_size=batchSize, verbose=1)\n",
        "print('Train loss:', trainLossCL)\n",
        "print('Train accuracy:', trainAccCL)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y6pIdwG6Zru-"
      },
      "source": [
        "#modelCL.save(CLModelTxt)\n",
        "#modelCL.save(CLModelH)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vcb5OtVf2cBB"
      },
      "source": [
        "modelCL.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p4aXRgoPSVyH"
      },
      "source": [
        "plt.title('Accuracy')\n",
        "plt.plot(historyCL.history['accuracy'], label='train')\n",
        "plt.plot(historyCL.history['val_accuracy'], label='test')\n",
        "plt.legend()\n",
        "plt.show();"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "llxtTnsWqeFQ"
      },
      "source": [
        "plt.title('Loss')\n",
        "plt.plot(historyCL.history['loss'], label='train')\n",
        "plt.plot(historyCL.history['val_loss'], label='test')\n",
        "plt.legend()\n",
        "plt.show();"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WQIe0TwHaNh9"
      },
      "source": [
        "y_val_pred_cl=modelCL.predict_classes(x_test_pad)\n",
        "precisionCL, recallCL, fscoreCL, supportCL = score(y_test, y_val_pred_cl)\n",
        "\n",
        "print('precision: {}'.format(precisionCL))\n",
        "print('recall: {}'.format(recallCL))\n",
        "print('fscore: {}'.format(fscoreCL))\n",
        "print('support: {}'.format(supportCL))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KzXf1PliAZdI"
      },
      "source": [
        "from sklearn.metrics import cohen_kappa_score\n",
        "\n",
        "cohen_score = cohen_kappa_score(y_test, y_val_pred_cl)\n",
        "\n",
        "print(cohen_score)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u3vqpI5_4aaH"
      },
      "source": [
        "# LSTM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "10qSvJ2E4gmV"
      },
      "source": [
        "max_features = numWords\n",
        "\n",
        "# LSTM\n",
        "\n",
        "batchSize = 16\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#\n",
        "print('Build model...')\n",
        "modelLSTM = Sequential()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "embeddingLayer = Embedding( numWords,\n",
        "                           embeddingDim,\n",
        "                           embeddings_initializer = Constant( embeddingMatrix),\n",
        "                           input_length = maxLen,\n",
        "                           trainable = isTrainable)\n",
        "modelLSTM.add(embeddingLayer)\n",
        "modelLSTM.add(Dropout(0.1))\n",
        "modelLSTM.add(LSTM(10, dropout=0.1))\n",
        "\n",
        "modelLSTM.add(Dropout(0.1))\n",
        "\n",
        "modelLSTM.add(Dense(1,activation='sigmoid'))\n",
        "\n",
        "\n",
        "modelLSTM.compile(loss='binary_crossentropy',optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "\n",
        "\n",
        "#modelLSTM.summary()\n",
        "\n",
        "rlrp = ReduceLROnPlateau(monitor='val_loss',\n",
        "                                  factor=0.1,\n",
        "                                  patience=8,\n",
        "                                  verbose=1,\n",
        "                                  mode='auto',\n",
        "                                  min_delta=0.0001,\n",
        "                                  cooldown=8,\n",
        "                                  min_lr=0)\n",
        "\n",
        "historyLSTM = modelLSTM.fit(x_train_pad, y_train,\n",
        "          batch_size=batchSize,\n",
        "          epochs=epochsNum,\n",
        "          validation_data=(x_test_pad, y_test),  verbose = 2)#, callbacks=[rlrp])\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CWXJ291CrWSu"
      },
      "source": [
        "testLossLSTM, testAccLSTM = modelLSTM.evaluate(x_test_pad, y_test,\n",
        "                            batch_size=batchSize)\n",
        "print('Test loss:', testLossLSTM)\n",
        "print('Test accuracy:', testAccLSTM)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d7kwIayU4Cep"
      },
      "source": [
        "trainLossLSTM, trainAccLSTM = modelLSTM.evaluate(x_train_pad, y_train, batch_size=batchSize, verbose=1)\n",
        "print('Train loss:', trainLossLSTM)\n",
        "print('Train accuracy:', trainAccLSTM)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jRTpj3OA4xV1"
      },
      "source": [
        "#modelLSTM.save(lstmModelTxt)\n",
        "#modelLSTM.save(lstmModelH)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aBWVGhXl4yxU"
      },
      "source": [
        "modelLSTM.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p-2rj9vr41Sv"
      },
      "source": [
        "plt.title('Accuracy')\n",
        "plt.plot(historyLSTM.history['accuracy'], label='train')\n",
        "plt.plot(historyLSTM.history['val_accuracy'], label='test')\n",
        "plt.legend()\n",
        "plt.show();"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YOWgLfwv47s7"
      },
      "source": [
        "plt.title('Loss')\n",
        "plt.plot(historyLSTM.history['loss'], label='train')\n",
        "plt.plot(historyLSTM.history['val_loss'], label='test')\n",
        "plt.legend()\n",
        "plt.show();"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oTtwEsm94_Zo"
      },
      "source": [
        "y_val_pred_l=modelLSTM.predict_classes(x_test_pad)\n",
        "precisionLSTM, recallLSTM, fscoreLSTM, supportLSTM = score(y_test, y_val_pred_l)\n",
        "\n",
        "print('precision: {}'.format(precisionLSTM))\n",
        "print('recall: {}'.format(recallLSTM))\n",
        "print('fscore: {}'.format(fscoreLSTM))\n",
        "print('support: {}'.format(supportLSTM))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RiBDb-2fA56F"
      },
      "source": [
        "from sklearn.metrics import cohen_kappa_score\n",
        "\n",
        "cohen_score = cohen_kappa_score(y_test, y_val_pred_l)\n",
        "\n",
        "print(cohen_score)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "byUw4FUoJeWg"
      },
      "source": [
        "# GRU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BGfuOfCpJg9c"
      },
      "source": [
        "print('Build model...')\n",
        "\n",
        "\n",
        "modelGRU = Sequential()\n",
        "\n",
        "# we start off with an efficient embedding layer which maps\n",
        "# our vocab indices into embedding_dims dimensions\n",
        "embeddingLayer = Embedding( numWords,\n",
        "                           embeddingDim,\n",
        "                           embeddings_initializer = Constant( embeddingMatrix),\n",
        "                           input_length = maxLen,\n",
        "                           trainable = isTrainable)\n",
        "\n",
        "modelGRU.add(embeddingLayer)\n",
        "\n",
        "modelGRU.add(Dropout(0.1))\n",
        "modelGRU.add(GRU(units= 10 , dropout = 0.1))\n",
        "\n",
        "modelGRU.add(Dropout(0.1))\n",
        "modelGRU.add(Dense(1,activation='sigmoid'))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "modelGRU.compile(loss='binary_crossentropy',\n",
        "              optimizer= 'adam',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "modelGRU.summary()\n",
        "\n",
        "\n",
        "rlrp = ReduceLROnPlateau(monitor='val_loss',\n",
        "                                  factor=0.1,\n",
        "                                  patience=8,\n",
        "                                  verbose=1,\n",
        "                                  mode='auto',\n",
        "                                  min_delta=0.0001,\n",
        "                                  cooldown=8,\n",
        "                                  min_lr=0)\n",
        "\n",
        "historyGRU = modelGRU.fit(x_train_pad, y_train,\n",
        "          batch_size=batchSize,\n",
        "          epochs= epochsNum,\n",
        "          validation_data=(x_test_pad, y_test),\n",
        "          verbose = 2)  # , callbacks=[rlrp])\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SiejgZLCra1u"
      },
      "source": [
        "testLossGRU, testAccGRU = modelGRU.evaluate(x_test_pad, y_test,\n",
        "                            batch_size=batchSize)\n",
        "print('Test loss:', testLossGRU)\n",
        "print('Test accuracy:', testAccGRU)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "10AUJp9b6snd"
      },
      "source": [
        "trainLossGRU, trainAccGRU = modelGRU.evaluate(x_train_pad, y_train,\n",
        "                            batch_size=batchSize)\n",
        "print('Train loss:', trainLossGRU)\n",
        "print('Train accuracy:', trainAccGRU)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5YBithYlJz-A"
      },
      "source": [
        "#modelGRU.save(gruModelTxt)\n",
        "#modelGRU.save(gruModelH)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u2nIq98HLy77"
      },
      "source": [
        "modelGRU.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zFb4PMFoL1Ov"
      },
      "source": [
        "plt.title('Accuracy')\n",
        "plt.plot(historyGRU.history['accuracy'], label='train')\n",
        "plt.plot(historyGRU.history['val_accuracy'], label='test')\n",
        "plt.legend()\n",
        "plt.show();"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WUVeBh-jL36o"
      },
      "source": [
        "plt.title('Loss')\n",
        "plt.plot(historyGRU.history['loss'], label='train')\n",
        "plt.plot(historyGRU.history['val_loss'], label='test')\n",
        "plt.legend()\n",
        "plt.show();"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6VwMh0jZMDZY"
      },
      "source": [
        "from sklearn.metrics import precision_recall_fscore_support as score\n",
        "\n",
        "\n",
        "y_val_pred_g=modelGRU.predict_classes(x_test_pad)\n",
        "precisionGRU, recallGRU, fscoreGRU, supportGRU = score(y_test, y_val_pred_g)\n",
        "\n",
        "print('precision: {}'.format(precisionGRU))\n",
        "print('recall: {}'.format(recallGRU))\n",
        "print('fscore: {}'.format(fscoreGRU))\n",
        "print('support: {}'.format(supportGRU))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L-MGBTQ7BVIn"
      },
      "source": [
        "from sklearn.metrics import cohen_kappa_score\n",
        "\n",
        "cohen_score = cohen_kappa_score(y_test, y_val_pred_g)\n",
        "\n",
        "print(cohen_score)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZE1_VRM2fnXa"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HV0iiCaifvJW"
      },
      "source": [
        "# BIGRU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6EnvQewffvJd"
      },
      "source": [
        "print('Build model...')\n",
        "\n",
        "\n",
        "modelBIGRU = Sequential()\n",
        "\n",
        "# we start off with an efficient embedding layer which maps\n",
        "# our vocab indices into embedding_dims dimensions\n",
        "embeddingLayer = Embedding( numWords,\n",
        "                           embeddingDim,\n",
        "                           embeddings_initializer = Constant( embeddingMatrix),\n",
        "                           input_length = maxLen,\n",
        "                           trainable = isTrainable)\n",
        "\n",
        "modelBIGRU.add(embeddingLayer)\n",
        "modelBIGRU.add(Dropout(0.1))\n",
        "modelBIGRU.add(Bidirectional(GRU(units=10, dropout = 0.1)))\n",
        "modelBIGRU.add(BatchNormalization())\n",
        "\n",
        "modelBIGRU.add(Dropout(0.1))\n",
        "modelBIGRU.add(Dense(1,activation='sigmoid'))\n",
        "\n",
        "\n",
        "\n",
        "modelBIGRU.compile(loss='binary_crossentropy',\n",
        "              optimizer= 'adam',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "\n",
        "historyBIGRU = modelBIGRU.fit(x_train_pad, y_train,\n",
        "          batch_size=batchSize,\n",
        "          epochs=epochsNum,\n",
        "          validation_data=(x_test_pad, y_test),\n",
        "          verbose = 2)  # , callbacks=[rlrp])\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gMELg0s7fvJ6"
      },
      "source": [
        "testLossBIGRU, testAccBIGRU = modelBIGRU.evaluate(x_test_pad, y_test,\n",
        "                            batch_size=batchSize)\n",
        "print('Test loss:', testLossBIGRU)\n",
        "print('Test accuracy:', testAccBIGRU)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IP6OaOiZfvKI"
      },
      "source": [
        "trainLossBIGRU, trainAccBIGRU = modelBIGRU.evaluate(x_train_pad, y_train,\n",
        "                            batch_size=batchSize)\n",
        "print('Train loss:', trainLossBIGRU)\n",
        "print('Train accuracy:', trainAccBIGRU)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vJxSN5W0fvKk"
      },
      "source": [
        "modelBIGRU.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LkfTedkRfvKy"
      },
      "source": [
        "plt.title('Accuracy')\n",
        "plt.plot(historyBIGRU.history['accuracy'], label='train')\n",
        "plt.plot(historyBIGRU.history['val_accuracy'], label='test')\n",
        "plt.legend()\n",
        "plt.show();"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IzQ23MPTfvK_"
      },
      "source": [
        "plt.title('Loss')\n",
        "plt.plot(historyBIGRU.history['loss'], label='train')\n",
        "plt.plot(historyBIGRU.history['val_loss'], label='test')\n",
        "plt.legend()\n",
        "plt.show();"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import precision_recall_fscore_support as score\n",
        "\n",
        "\n",
        "y_val_pred_bg=modelBIGRU.predict_classes(x_test_pad)\n",
        "precisionbGRU, recallbGRU, fscorebGRU, supportbGRU = score(y_test, y_val_pred_bg)\n",
        "\n",
        "print('precision: {}'.format(precisionbGRU))\n",
        "print('recall: {}'.format(recallbGRU))\n",
        "print('fscore: {}'.format(fscorebGRU))\n",
        "print('support: {}'.format(supportbGRU))"
      ],
      "metadata": {
        "id": "lMXmJFK_vGaz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xCk3y2wMCYIF"
      },
      "source": [
        "from sklearn.metrics import cohen_kappa_score\n",
        "\n",
        "cohen_score = cohen_kappa_score(y_test, y_val_pred_bg)\n",
        "\n",
        "print(cohen_score)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W8v3DRX0MPJF"
      },
      "source": [
        "# BILSTM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ridZiLWYMa23"
      },
      "source": [
        "maxFeatures = numWords\n",
        "\n",
        "unitsNumber = 20\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#model.compile(..., optimizer=opt)\n",
        "print('Build model...')\n",
        "modelBiLSTM = Sequential()\n",
        "\n",
        "# we start off with an efficient embedding layer which maps\n",
        "# our vocab indices into embedding_dims dimensions\n",
        "embeddingLayer = Embedding( numWords,\n",
        "                           embeddingDim,\n",
        "                           embeddings_initializer = Constant( embeddingMatrix),\n",
        "                           input_length = maxLen,\n",
        "                           trainable = isTrainable)\n",
        "\n",
        "modelBiLSTM.add(embeddingLayer)\n",
        "\n",
        "modelBiLSTM.add(Dropout(0.1))\n",
        "\n",
        "modelBiLSTM.add(Bidirectional(LSTM(10, dropout = 0.1)))\n",
        "\n",
        "modelBiLSTM.add(Dropout(0.1))\n",
        "\n",
        "modelBiLSTM.add(Dense(1,activation='sigmoid'))\n",
        "\n",
        "modelBiLSTM.compile(loss='binary_crossentropy',optimizer= 'adam', metrics=['accuracy'])\n",
        "\n",
        "\n",
        "#modelBiLSTM.summary()\n",
        "\n",
        "rlrp = ReduceLROnPlateau(monitor='val_loss',\n",
        "                                  factor=0.1,\n",
        "                                  patience=8,\n",
        "                                  verbose=1,\n",
        "                                  mode='auto',\n",
        "                                  min_delta=0.0001,\n",
        "                                  cooldown=8,\n",
        "                                  min_lr=0)\n",
        "\n",
        "historyBILSTM = modelBiLSTM.fit(x_train_pad, y_train,\n",
        "          batch_size=batchSize,\n",
        "          epochs=epochsNum,\n",
        "          validation_data=(x_test_pad, y_test),\n",
        "          verbose = 2 )#, callbacks=[rlrp])\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h8d7UL68r9e-"
      },
      "source": [
        "testLossBilstm, testAccBilstm = modelBiLSTM.evaluate(x_test_pad, y_test,\n",
        "                            batch_size=batchSize)\n",
        "print('Test score:', testLossBilstm)\n",
        "print('Test accuracy:', testAccBilstm)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZKj71OeY8eI4"
      },
      "source": [
        "trainLossBilstm, trainAccBilstm = modelBiLSTM.evaluate(x_train_pad, y_train,\n",
        "                            batch_size=batchSize)\n",
        "print('Train score:', trainLossBilstm)\n",
        "print('Train accuracy:', trainAccBilstm)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZGWWmQAvOAC1"
      },
      "source": [
        "#modelBiLSTM.save(bilstmModelTxt)\n",
        "#modelBiLSTM.save(bilstmModelH)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kugkgIgzPzQF"
      },
      "source": [
        "modelBiLSTM.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "13uhFiPWP0Dt"
      },
      "source": [
        "plt.title('Accuracy')\n",
        "plt.plot(historyBILSTM.history['accuracy'], label='train')\n",
        "plt.plot(historyBILSTM.history['val_accuracy'], label='test')\n",
        "plt.legend()\n",
        "plt.show();"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vh9R4pXGP28a"
      },
      "source": [
        "plt.title('Loss')\n",
        "plt.plot(historyBILSTM.history['loss'], label='train')\n",
        "plt.plot(historyBILSTM.history['val_loss'], label='test')\n",
        "plt.legend()\n",
        "plt.show();"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pZX8sUAbP7Bk"
      },
      "source": [
        "y_val_pred_bl=modelBiLSTM.predict_classes(x_test_pad)\n",
        "precisionBilstm, recallBilstm, fscoreBilstm, supportBilstm = score(y_test, y_val_pred_bl)\n",
        "\n",
        "print('precision: {}'.format(precisionBilstm))\n",
        "print('recall: {}'.format(recallBilstm))\n",
        "print('fscore: {}'.format(fscoreBilstm))\n",
        "print('support: {}'.format(supportBilstm))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BRUtpsvCDhI5"
      },
      "source": [
        "from sklearn.metrics import cohen_kappa_score\n",
        "\n",
        "cohen_score = cohen_kappa_score(y_test, y_val_pred_bl)\n",
        "\n",
        "print(cohen_score)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6gqt70t799a9"
      },
      "source": [
        "# CNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P38k_J2BrCRN"
      },
      "source": [
        "\n",
        "\n",
        "# set parameters:\n",
        "max_features = numWords\n",
        "#maxlen = 400\n",
        "\n",
        "\n",
        "print('Build model...')\n",
        "modelConv1 = Sequential()\n",
        "\n",
        "# we start off with an efficient embedding layer which maps\n",
        "# our vocab indices into embedding_dims dimensions\n",
        "modelConv1.add(Embedding(numWords, embeddingDim ,\n",
        "                      embeddings_initializer = Constant( embeddingMatrix),\n",
        "                      input_length=maxLen,\n",
        "                      trainable = isTrainable))\n",
        "\n",
        "# we add a Convolution1D, which will learn filters\n",
        "# word group filters of size filter_length:\n",
        "\n",
        "modelConv1.add(Conv1D(filters=filtersNum,\n",
        "                 kernel_size=kernelSize,\n",
        "                 padding='same',\n",
        "                 activation='relu',\n",
        "                 strides=1))\n",
        "modelConv1.add(Dropout(0.2))\n",
        "\n",
        "modelConv1.add(MaxPooling1D(pool_size=2))\n",
        "modelConv1.add(Dropout(0.1))\n",
        "\n",
        "modelConv1.add(Flatten())\n",
        "modelConv1.add(Dense(10, activation='relu'))\n",
        "modelConv1.add(Dropout(0.1))\n",
        "\n",
        "modelConv1.add(BatchNormalization())\n",
        "\n",
        "modelConv1.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "modelConv1.compile(loss='binary_crossentropy',\n",
        "              optimizer= 'adam',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "#modelConv1.summary()\n",
        "\n",
        "\n",
        "historyConv1=modelConv1.fit(x_train_pad, y_train,\n",
        "          batch_size=batchSize,\n",
        "          epochs=epochsNum,\n",
        "          validation_data=(x_test_pad, y_test), verbose = 2)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yvJPRApbsd4i"
      },
      "source": [
        "testLossConv, testAccConv = modelConv1.evaluate(x_test_pad, y_test, batch_size=batchSize)\n",
        "print('Test loss:', testLossConv)\n",
        "print('Test accuracy:', testAccConv)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3XBuMPYg9WUQ"
      },
      "source": [
        "trainLossConv, trainAccConv = modelConv1.evaluate(x_train_pad, y_train, batch_size=batchSize)\n",
        "print('Train loss:', trainLossConv)\n",
        "print('Train accuracy:', trainAccConv)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VFfT1l4U-FSx"
      },
      "source": [
        "#modelConv1.save(conv1ModelTxt)\n",
        "#modelConv1.save(conv1mModelH)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bwftiDG0-gvR"
      },
      "source": [
        "modelConv1.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7TjeuK4v-kPK"
      },
      "source": [
        "plt.title('Accuracy')\n",
        "plt.plot(historyConv1.history['accuracy'], label='train')\n",
        "plt.plot(historyConv1.history['val_accuracy'], label='test')\n",
        "plt.legend()\n",
        "plt.show();"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "czwbnbuq-k3D"
      },
      "source": [
        "plt.title('Loss')\n",
        "plt.plot(historyConv1.history['loss'], label='train')\n",
        "plt.plot(historyConv1.history['val_loss'], label='test')\n",
        "plt.legend()\n",
        "plt.show();"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SW0SSZbB-p_9"
      },
      "source": [
        "y_val_pred_c =modelConv1.predict_classes(x_test_pad)\n",
        "precisionConv1, recallConv1, fscoreConv1, supportConv1 = score(y_test, y_val_pred_c)\n",
        "\n",
        "print('precision: {}'.format(precisionConv1))\n",
        "print('recall: {}'.format(recallConv1))\n",
        "print('fscore: {}'.format(fscoreConv1))\n",
        "print('support: {}'.format(supportConv1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Hu6Ccx4EMyO"
      },
      "source": [
        "from sklearn.metrics import cohen_kappa_score\n",
        "\n",
        "cohen_score = cohen_kappa_score(y_test, y_val_pred_c)\n",
        "\n",
        "print(cohen_score)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sRzssXDYCI-f"
      },
      "source": [
        "# Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uPTewWYOGtT0"
      },
      "source": [
        "## Accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aq0pbFrLCM67"
      },
      "source": [
        "dicTestAccuracy={\n",
        "      \"LSTM\"      :(\"{:.4f}\".format(testAccLSTM)),\n",
        "      \"Bilstm\":(\"{:.4f}\".format(testAccBilstm)),\n",
        "      \"GRU\"      :(\"{:.4f}\".format(testAccGRU)),\n",
        "      \"CNN\"     :(\"{:.4f}\".format(testAccConv)),\n",
        "      \"CNN+LSTM\"      :(\"{:.4f}\".format(testAccCL)),\n",
        "      \"CNN+GRU\"      :(\"{:.4f}\".format(testAccCGRU)),\n",
        "      \"CNN+BiLSTM\"      :(\"{:.4f}\".format(testAccCBiLSTM))\n",
        "      }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "asqlP5OjCxSt"
      },
      "source": [
        "dfTestAccuracy = pd.DataFrame.from_dict(dicTestAccuracy, orient='index', columns=[\"Accuracy\"] )\n",
        "dfTestAccuracy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gCfheXn3Ggv7"
      },
      "source": [
        "dfTestAccuracy[\"Accuracy\"] = dfTestAccuracy[\"Accuracy\"].astype(float)\n",
        "dfTestAccuracy[\"Accuracy\"].plot(kind=\"bar\", grid=True , color=tuple([\"g\", \"b\", \"r\", \"y\", \"k\"]), width=0.3)\n",
        "plt.title(\"Accuracy\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eX1hY2cdGwv3"
      },
      "source": [
        "## Precision"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nRS3G7w4bI9G"
      },
      "source": [
        "precisionLSTM"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4QPgyTPjG4Ip"
      },
      "source": [
        "dicPrecision={\n",
        "      \"LSTM\"      :(\"{:.4f}\".format(precisionLSTM[0])),\n",
        "      \"Bilstm\":(\"{:.4f}\".format(precisionBilstm[0])),\n",
        "      \"GRU\"      :(\"{:.4f}\".format(precisionGRU[0])),\n",
        "      \"CNN\"     :(\"{:.4f}\".format(precisionConv1[0])),\n",
        "      \"CNN+LSTM\"      :(\"{:.4f}\".format(precisionCL[0])),\n",
        "      \"CNN+GRU\"      :(\"{:.4f}\".format(precisionCGRU[0])),\n",
        "      \"CNN+BiLSTM\"      :(\"{:.4f}\".format(precisionCBiLSTM[0]))\n",
        "      }\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ep1NaC5-HcgX"
      },
      "source": [
        "dfPrecision = pd.DataFrame.from_dict(dicPrecision, orient='index', columns=[\"Precision\"] )\n",
        "dfPrecision"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9zBwERtVHela"
      },
      "source": [
        "dfPrecision[\"Precision\"] = dfPrecision[\"Precision\"].astype(float)\n",
        "dfPrecision[\"Precision\"].plot(kind=\"bar\", grid=True , color=tuple([\"g\", \"b\", \"r\", \"y\", \"k\"]), width=0.3)\n",
        "plt.title(\"Precision\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QZXeaWWaIPnQ"
      },
      "source": [
        "## Recall"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eukDAiFVISpT"
      },
      "source": [
        "dicRecall={\n",
        "      \"LSTM\"      :(\"{:.4f}\".format(recallLSTM[0])),\n",
        "      \"Bilstm\":(\"{:.4f}\".format(recallBilstm[0])),\n",
        "      \"GRU\"      :(\"{:.4f}\".format(recallGRU[0])),\n",
        "      \"CNN\"     :(\"{:.4f}\".format(recallConv1[0])),\n",
        "      \"CNN+LSTM\"      :(\"{:.4f}\".format(recallCL[0])),\n",
        "      \"CNN+GRU\"      :(\"{:.4f}\".format(recallCGRU[0])),\n",
        "      \"CNN+BiLSTM\"      :(\"{:.4f}\".format(recallCBiLSTM[0]))\n",
        "      }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3w591WirIUrN"
      },
      "source": [
        "dfRecall = pd.DataFrame.from_dict(dicRecall, orient='index', columns=[\"Recall\"] )\n",
        "dfRecall"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Io6r23LtIZuq"
      },
      "source": [
        "dfRecall[\"Recall\"] = dfRecall[\"Recall\"].astype(float)\n",
        "dfRecall[\"Recall\"].plot(kind=\"bar\", grid=True , color=tuple([\"g\", \"b\", \"r\", \"y\", \"k\"]), width=0.3)\n",
        "plt.title(\"Recall\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Beq11JiYJB54"
      },
      "source": [
        "## Fscore"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MTaSeEtzJFWc"
      },
      "source": [
        "dicFscore={\n",
        "      \"LSTM\"      :(\"{:.4f}\".format(fscoreLSTM[0])),\n",
        "      \"Bilstm\":(\"{:.4f}\".format(fscoreBilstm[0])),\n",
        "      \"GRU\"      :(\"{:.4f}\".format(fscoreGRU[0])),\n",
        "      \"CNN\"     :(\"{:.4f}\".format(fscoreConv1[0])),\n",
        "      \"CNN+LSTM\"      :(\"{:.4f}\".format(fscoreCL[0])),\n",
        "      \"CNN+GRU\"      :(\"{:.4f}\".format(fscoreCGRU[0])),\n",
        "      \"CNN+BiLSTM\"      :(\"{:.4f}\".format(fscoreCBiLSTM[0]))\n",
        "      }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nh2EXieZJJwx"
      },
      "source": [
        "dfFscore = pd.DataFrame.from_dict(dicFscore, orient='index', columns=[\"Fscore\"] )\n",
        "dfFscore"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VHmGS-GcJLkV"
      },
      "source": [
        "dfFscore[\"Fscore\"] = dfFscore[\"Fscore\"].astype(float)\n",
        "dfFscore[\"Fscore\"].plot(kind=\"bar\", grid=True , color=tuple([\"g\", \"b\", \"r\", \"y\", \"k\"]), width=0.3)\n",
        "plt.title(\"Fscore\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-CQaBfqPeVZN"
      },
      "source": [
        "##ALL"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "positives"
      ],
      "metadata": {
        "id": "BDIWOrip-fhE"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CP7uCkl2eYNL"
      },
      "source": [
        "dicALL={\n",
        "      \"Accuracy\":[\n",
        "                  (\"{:.4f}\".format(testAccLSTM)),\n",
        "                  (\"{:.4f}\".format(testAccBilstm)),\n",
        "                  (\"{:.4f}\".format(testAccGRU)),\n",
        "                  (\"{:.4f}\".format(testAccBIGRU)),\n",
        "                  (\"{:.4f}\".format(testAccConv)),\n",
        "                  (\"{:.4f}\".format(testAccCL)),\n",
        "                  (\"{:.4f}\".format(testAccCGRU)),\n",
        "                  (\"{:.4f}\".format(testAccCBiLSTM)),\n",
        "                  (\"{:.4f}\".format(testAccCBIGRU))\n",
        "                 ],\n",
        "\n",
        "      \"Precision\":[\n",
        "                  (\"{:.4f}\".format(precisionLSTM[0])),\n",
        "                  (\"{:.4f}\".format(precisionBilstm[0])),\n",
        "                  (\"{:.4f}\".format(precisionGRU[0])),\n",
        "                  (\"{:.4f}\".format(precisionbGRU[0])),\n",
        "                  (\"{:.4f}\".format(precisionConv1[0])),\n",
        "                  (\"{:.4f}\".format(precisionCL[0])),\n",
        "                  (\"{:.4f}\".format(precisionCGRU[0])),\n",
        "                  (\"{:.4f}\".format(precisionCBiLSTM[0])),\n",
        "                  (\"{:.4f}\".format(precisionCBIGRU[0]))\n",
        "                 ],\n",
        "\n",
        "      \"Recall\":[\n",
        "                (\"{:.4f}\".format(recallLSTM[0])),\n",
        "                (\"{:.4f}\".format(recallBilstm[0])),\n",
        "                (\"{:.4f}\".format(recallGRU[0])),\n",
        "                (\"{:.4f}\".format(recallbGRU[0])),\n",
        "                (\"{:.4f}\".format(recallConv1[0])),\n",
        "                (\"{:.4f}\".format(recallCL[0])),\n",
        "                (\"{:.4f}\".format(recallCGRU[0])),\n",
        "                (\"{:.4f}\".format(recallCBiLSTM[0])),\n",
        "                (\"{:.4f}\".format(recallCBIGRU[0]))\n",
        "                 ],\n",
        "\n",
        "      \"F1\":[\n",
        "            (\"{:.4f}\".format(fscoreLSTM[0])),\n",
        "            (\"{:.4f}\".format(fscoreBilstm[0])),\n",
        "            (\"{:.4f}\".format(fscoreGRU[0])),\n",
        "            (\"{:.4f}\".format(fscorebGRU[0])),\n",
        "            (\"{:.4f}\".format(fscoreConv1[0])),\n",
        "            (\"{:.4f}\".format(fscoreCL[0])),\n",
        "            (\"{:.4f}\".format(fscoreCGRU[0])),\n",
        "            (\"{:.4f}\".format(fscoreCBiLSTM[0])),\n",
        "            (\"{:.4f}\".format(fscoreCBIGRU[0]))\n",
        "           ],\n",
        "     \"Loss\":[\n",
        "            (\"{:.4f}\".format(testLossLSTM)),\n",
        "            (\"{:.4f}\".format(testLossBilstm)),\n",
        "            (\"{:.4f}\".format(testLossGRU)),\n",
        "            (\"{:.4f}\".format(testLossBIGRU)),\n",
        "            (\"{:.4f}\".format(testLossConv)),\n",
        "            (\"{:.4f}\".format(testLossCL)),\n",
        "            (\"{:.4f}\".format(testLossCGRU)),\n",
        "            (\"{:.4f}\".format(testLossCBiLSTM)),\n",
        "            (\"{:.4f}\".format(testLossCBIGRU))\n",
        "           ]\n",
        "\n",
        "\n",
        "        }\n",
        "\n",
        "\n",
        "dfAllMeasures=pd.DataFrame.from_dict( dicALL , orient='index')#,index=['SIMPLE RNN','LSTM MODEL','GRU MODEL','SVR MODEL','SARIMAX MODEL'],\n",
        "                                 #columns=[\"MSE\",\"MAE\"] )\n",
        "dfAllMeasures.columns=['LSTM','Bilstm','GRU', 'BIGRU','Conv1','CNN+LSTM', 'CNN+GRU', 'CNN+BiLSTM', 'CNN+BIGRU']\n",
        "dfAllMeasures = dfAllMeasures.T\n",
        "dfAllMeasures"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "negatives"
      ],
      "metadata": {
        "id": "5bS_l8h4-cdx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dicALL={\n",
        "      \"Accuracy\":[\n",
        "                  (\"{:.4f}\".format(testAccLSTM)),\n",
        "                  (\"{:.4f}\".format(testAccBilstm)),\n",
        "                  (\"{:.4f}\".format(testAccGRU)),\n",
        "                  (\"{:.4f}\".format(testAccBIGRU)),\n",
        "                  (\"{:.4f}\".format(testAccConv)),\n",
        "                  (\"{:.4f}\".format(testAccCL)),\n",
        "                  (\"{:.4f}\".format(testAccCGRU)),\n",
        "                  (\"{:.4f}\".format(testAccCBiLSTM)),\n",
        "                  (\"{:.4f}\".format(testAccCBIGRU))\n",
        "                 ],\n",
        "\n",
        "      \"Precision\":[\n",
        "                  (\"{:.4f}\".format(precisionLSTM[1])),\n",
        "                  (\"{:.4f}\".format(precisionBilstm[1])),\n",
        "                  (\"{:.4f}\".format(precisionGRU[1])),\n",
        "                  (\"{:.4f}\".format(precisionbGRU[1])),\n",
        "                  (\"{:.4f}\".format(precisionConv1[1])),\n",
        "                  (\"{:.4f}\".format(precisionCL[1])),\n",
        "                  (\"{:.4f}\".format(precisionCGRU[1])),\n",
        "                  (\"{:.4f}\".format(precisionCBiLSTM[1])),\n",
        "                  (\"{:.4f}\".format(precisionCBIGRU[1]))\n",
        "                 ],\n",
        "\n",
        "      \"Recall\":[\n",
        "                (\"{:.4f}\".format(recallLSTM[1])),\n",
        "                (\"{:.4f}\".format(recallBilstm[1])),\n",
        "                (\"{:.4f}\".format(recallGRU[1])),\n",
        "                (\"{:.4f}\".format(recallbGRU[1])),\n",
        "                (\"{:.4f}\".format(recallConv1[1])),\n",
        "                (\"{:.4f}\".format(recallCL[1])),\n",
        "                (\"{:.4f}\".format(recallCGRU[1])),\n",
        "                (\"{:.4f}\".format(recallCBiLSTM[1])),\n",
        "                (\"{:.4f}\".format(recallCBIGRU[1]))\n",
        "                 ],\n",
        "\n",
        "      \"F1\":[\n",
        "            (\"{:.4f}\".format(fscoreLSTM[1])),\n",
        "            (\"{:.4f}\".format(fscoreBilstm[1])),\n",
        "            (\"{:.4f}\".format(fscoreGRU[1])),\n",
        "            (\"{:.4f}\".format(fscorebGRU[1])),\n",
        "            (\"{:.4f}\".format(fscoreConv1[1])),\n",
        "            (\"{:.4f}\".format(fscoreCL[1])),\n",
        "            (\"{:.4f}\".format(fscoreCGRU[1])),\n",
        "            (\"{:.4f}\".format(fscoreCBiLSTM[1])),\n",
        "            (\"{:.4f}\".format(fscoreCBIGRU[1]))\n",
        "           ],\n",
        "     \"Loss\":[\n",
        "            (\"{:.4f}\".format(testLossLSTM)),\n",
        "            (\"{:.4f}\".format(testLossBilstm)),\n",
        "            (\"{:.4f}\".format(testLossGRU)),\n",
        "            (\"{:.4f}\".format(testLossBIGRU)),\n",
        "            (\"{:.4f}\".format(testLossConv)),\n",
        "            (\"{:.4f}\".format(testLossCL)),\n",
        "            (\"{:.4f}\".format(testLossCGRU)),\n",
        "            (\"{:.4f}\".format(testLossCBiLSTM)),\n",
        "            (\"{:.4f}\".format(testLossCBIGRU))\n",
        "           ]\n",
        "\n",
        "\n",
        "        }\n",
        "\n",
        "\n",
        "dfAllMeasures=pd.DataFrame.from_dict( dicALL , orient='index')#,index=['SIMPLE RNN','LSTM MODEL','GRU MODEL','SVR MODEL','SARIMAX MODEL'],\n",
        "                                 #columns=[\"MSE\",\"MAE\"] )\n",
        "dfAllMeasures.columns=['LSTM','Bilstm','GRU', 'BIGRU','Conv1','CNN+LSTM', 'CNN+GRU', 'CNN+BiLSTM', 'CNN+BIGRU']\n",
        "dfAllMeasures = dfAllMeasures.T\n",
        "dfAllMeasures"
      ],
      "metadata": {
        "id": "SdzMMg7v0FOl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}