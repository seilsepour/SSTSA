{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/seilsepour/SSTSA/blob/main/Preprocessing_Module.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yT7Rfe0eKG3x"
      },
      "source": [
        "This module is resposible for\n",
        "\n",
        "\n",
        "*   Preprocessing\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4CZ8jTPXsX-E"
      },
      "source": [
        "# Configuration and imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1WxrHiDfKG4V"
      },
      "outputs": [],
      "source": [
        "pip install spacy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lRmIqv4K6R9W"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "import pprint\n",
        "import time\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.corpus import words\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from gensim import corpora, models, similarities\n",
        "from itertools import chain\n",
        "from collections import Counter\n",
        "import spacy\n",
        "import gensim, logging\n",
        "import nltk\n",
        "from time import time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import string\n",
        "from nltk.corpus import wordnet as wnpi\n",
        "import pickle\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "\n",
        "import re\n",
        "from functools import partial\n",
        "from collections import Counter\n",
        "import nltk\n",
        "from nltk.corpus import wordnet\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "\n",
        "\n",
        "#import techniques\n",
        "#from techniques import *\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('words')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qo5fkNy0slaZ"
      },
      "outputs": [],
      "source": [
        "txtSpellFile = '/MYDRIVE/My Drive/Colab Notebooks/SecondPaper/data/corporaForSpellCorrection.txt'\n",
        "txtSlangFile = '/MYDRIVE/My Drive/Colab Notebooks/SecondPaper/data/slang.txt'\n",
        "csvDestinationFileName = '/MYDRIVE/My Drive/Colab Notebooks/SecondPaper/data/PreProcessedPang-Ver2-00-11-27.csv'\n",
        "csvSourceFileName = '/MYDRIVE/My Drive/Colab Notebooks/SecondPaper/data/Pang-Ver2-00-10-24.csv'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hK5wVyhfssBQ"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/MYDRIVE', force_remount=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y0RVxyoVmpGs"
      },
      "source": [
        "# commpn functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gjRRHjkyBprM"
      },
      "outputs": [],
      "source": [
        "\n",
        "\"\"\" Creates a dictionary with slangs and their equivalents and replaces them \"\"\"\n",
        "with open(txtSlangFile) as file:\n",
        "    slang_map = dict(map(str.strip, line.partition('\\t')[::2])\n",
        "    for line in file if line.strip())\n",
        "\n",
        "slang_words = sorted(slang_map, key=len, reverse=True) # longest first for regex\n",
        "regex = re.compile(r\"\\b({})\\b\".format(\"|\".join(map(re.escape, slang_words))))\n",
        "replaceSlang = partial(regex.sub, lambda m: slang_map[m.group(1)])\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_b-wIpdmCZr0"
      },
      "outputs": [],
      "source": [
        "\"\"\" Replaces contractions from a string to their equivalents \"\"\"\n",
        "contraction_patterns = [ (r'won\\'t', 'will not'), (r'can\\'t', 'cannot'), (r'i\\'m', 'i am'), (r'ain\\'t', 'is not'), (r'(\\w+)\\'ll', '\\g<1> will'), (r'(\\w+)n\\'t', '\\g<1> not'),\n",
        "                         (r'(\\w+)\\'ve', '\\g<1> have'), (r'(\\w+)\\'s', '\\g<1> is'), (r'(\\w+)\\'re', '\\g<1> are'), (r'(\\w+)\\'d', '\\g<1> would'), (r'&', 'and'), (r'dammit', 'damn it'), (r'dont', 'do not'), (r'wont', 'will not') ]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FphdRandA9q4"
      },
      "outputs": [],
      "source": [
        "def removeUnicode(text):\n",
        "    \"\"\" Removes unicode strings like \"\\u002c\" and \"x96\" \"\"\"\n",
        "    text = re.sub(r'(\\\\u[0-9A-Fa-f]+)',r'', text)\n",
        "    text = re.sub(r'[^\\x00-\\x7f]',r'',text)\n",
        "    return text\n",
        "\n",
        "def replaceURL(text):\n",
        "    \"\"\" Replaces url address with \"url\" \"\"\"\n",
        "    #edited by seilsepour\n",
        "    text = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))','',text)\n",
        "    text = re.sub(r'#([^\\s]+)', r'\\1', text)\n",
        "    return text\n",
        "\n",
        "def replaceAtUser(text):\n",
        "    \"\"\" Replaces \"@user\" with \"atUser\" \"\"\"\n",
        "    #edited by seilsepour\n",
        "    #text = re.sub('@[^\\s]+','atUser',text)\n",
        "    text = re.sub('@[^\\s]+','',text)\n",
        "    return text\n",
        "\n",
        "def removeHashtagInFrontOfWord(text):\n",
        "    \"\"\" Removes hastag in front of a word \"\"\"\n",
        "    text = re.sub(r'#([^\\s]+)', r'\\1', text)\n",
        "    return text\n",
        "\n",
        "def removeNumbers(text):\n",
        "    \"\"\" Removes integers \"\"\"\n",
        "    text = ''.join([i for i in text if not i.isdigit()])\n",
        "    return text\n",
        "\n",
        "def replaceMultiExclamationMark(text):\n",
        "    \"\"\" Replaces repetitions of exlamation marks \"\"\"\n",
        "    #edited by seilsepour\n",
        "    #text = re.sub(r\"(\\!)\\1+\", ' multiExclamation ', text)\n",
        "    text = re.sub(r\"(\\!)\\1+\", '', text)\n",
        "    return text\n",
        "\n",
        "def replaceMultiQuestionMark(text):\n",
        "    \"\"\" Replaces repetitions of question marks \"\"\"\n",
        "    #edited by seilsepour\n",
        "    #text = re.sub(r\"(\\?)\\1+\", ' multiQuestion ', text)\n",
        "    text = re.sub(r\"(\\?)\\1+\", '', text)\n",
        "    return text\n",
        "\n",
        "def replaceMultiStopMark(text):\n",
        "    \"\"\" Replaces repetitions of stop marks \"\"\"\n",
        "    #edited by seilsepour\n",
        "    #text = re.sub(r\"(\\.)\\1+\", ' multiStop ', text)\n",
        "    text = re.sub(r\"(\\.)\\1+\", '', text)\n",
        "    return text\n",
        "\n",
        "def countMultiExclamationMarks(text):\n",
        "    \"\"\" Replaces repetitions of exlamation marks \"\"\"\n",
        "    return text #len(re.findall(r\"(\\!)\\1+\", text))\n",
        "\n",
        "def countMultiQuestionMarks(text):\n",
        "  return text\n",
        "  #return len(re.findall(r\"(\\?)\\1+\", text))\n",
        "\n",
        "def countMultiStopMarks(text):\n",
        "  return text\n",
        "  #return len(re.findall(r\"(\\.)\\1+\", text))\n",
        "\n",
        "def countElongated(text):\n",
        "  return text\n",
        "  #regex = re.compile(r\"(.)\\1{2}\")\n",
        "  #return len([word for word in text.split() if regex.search(word)])\n",
        "\n",
        "def countAllCaps(text):\n",
        "  return text\n",
        "  #return len(re.findall(\"[A-Z0-9]{3,}\", text))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oFJVrAvlBIiH"
      },
      "outputs": [],
      "source": [
        "def countSlang(text):\n",
        "    \"\"\" Input: a text, Output: how many slang words and a list of found slangs \"\"\"\n",
        "    slangCounter = 0\n",
        "    slangsFound = []\n",
        "    tokens = nltk.word_tokenize(text)\n",
        "    for word in tokens:\n",
        "        if word in slang_words:\n",
        "            slangsFound.append(word)\n",
        "            slangCounter += 1\n",
        "    return 0, 0 #slangCounter, slangsFound\n",
        "\n",
        "def replaceContraction(text):\n",
        "    patterns = [(re.compile(regex), repl) for (regex, repl) in contraction_patterns]\n",
        "    for (pattern, repl) in patterns:\n",
        "        (text, count) = re.subn(pattern, repl, text)\n",
        "    return text\n",
        "\n",
        "def replaceElongated(word):\n",
        "    \"\"\" Replaces an elongated word with its basic form, unless the word exists in the lexicon \"\"\"\n",
        "    #return word\n",
        "    repeat_regexp = re.compile(r'(\\w*)(\\w)\\2(\\w*)')\n",
        "    repl = r'\\1\\2\\3'\n",
        "    if wordnet.synsets(word):\n",
        "        return word\n",
        "    repl_word = repeat_regexp.sub(repl, word)\n",
        "    if repl_word != word:\n",
        "        return replaceElongated(repl_word)\n",
        "    else:\n",
        "        return repl_word\n",
        "\n",
        "def removeEmoticons(text):\n",
        "    \"\"\" Removes emoticons from text \"\"\"\n",
        "    text = re.sub(':\\)|;\\)|:-\\)|\\(-:|:-D|=D|:P|xD|X-p|\\^\\^|:-*|\\^\\.\\^|\\^\\-\\^|\\^\\_\\^|\\,-\\)|\\)-:|:\\'\\(|:\\(|:-\\(|:\\S|T\\.T|\\.\\_\\.|:<|:-\\S|:-<|\\*\\-\\*|:O|=O|=\\-O|O\\.o|XO|O\\_O|:-\\@|=/|:/|X\\-\\(|>\\.<|>=\\(|D:', '', text)\n",
        "    return text\n",
        "\n",
        "def countEmoticons(text):\n",
        "    \"\"\" Input: a text, Output: how many emoticons \"\"\"\n",
        "    return 0 #len(re.findall(':\\)|;\\)|:-\\)|\\(-:|:-D|=D|:P|xD|X-p|\\^\\^|:-*|\\^\\.\\^|\\^\\-\\^|\\^\\_\\^|\\,-\\)|\\)-:|:\\'\\(|:\\(|:-\\(|:\\S|T\\.T|\\.\\_\\.|:<|:-\\S|:-<|\\*\\-\\*|:O|=O|=\\-O|O\\.o|XO|O\\_O|:-\\@|=/|:/|X\\-\\(|>\\.<|>=\\(|D:', text))\n",
        "\n",
        "\n",
        "### Spell Correction begin ###\n",
        "\"\"\" Spell Correction http://norvig.com/spell-correct.html \"\"\"\n",
        "def words(text):\n",
        "  return re.findall(r'\\w+', text.lower())\n",
        "  #return text\n",
        "\n",
        "WORDS = Counter(words(open(txtSpellFile).read()))\n",
        "\n",
        "def P(word, N=sum(WORDS.values())):\n",
        "    return WORDS[word] / N\n",
        "\n",
        "def spellCorrection(word):\n",
        "    \"\"\" Most probable spelling correction for word. \"\"\"\n",
        "    return max(candidates(word), key=P)\n",
        "    #return word\n",
        "\n",
        "def candidates(word):\n",
        "    \"\"\" Generate possible spelling corrections for word. \"\"\"\n",
        "    return (known([word]) or known(edits1(word)) or known(edits2(word)) or [word])\n",
        "    #return word\n",
        "\n",
        "def known(words):\n",
        "    \"\"\" The subset of `words` that appear in the dictionary of WORDS. \"\"\"\n",
        "    return set(w for w in words if w in WORDS)\n",
        "    #return words\n",
        "\n",
        "def edits1(word):\n",
        "    \"\"\" All edits that are one edit away from `word`. \"\"\"\n",
        "    letters    = 'abcdefghijklmnopqrstuvwxyz'\n",
        "    splits     = [(word[:i], word[i:])    for i in range(len(word) + 1)]\n",
        "    deletes    = [L + R[1:]               for L, R in splits if R]\n",
        "    transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R)>1]\n",
        "    replaces   = [L + c + R[1:]           for L, R in splits if R for c in letters]\n",
        "    inserts    = [L + c + R               for L, R in splits for c in letters]\n",
        "    return set(deletes + transposes + replaces + inserts)\n",
        "\n",
        "def edits2(word):\n",
        "    \"\"\" All edits that are two edits away from `word`. \"\"\"\n",
        "    return (e2 for e1 in edits1(word) for e2 in edits1(e1))\n",
        "\n",
        "### Spell Correction End ###\n",
        "\n",
        "### Replace Negations Begin ###\n",
        "\n",
        "def replace(word, pos=None):\n",
        "    \"\"\" Creates a set of all antonyms for the word and if there is only one antonym, it returns it \"\"\"\n",
        "    antonyms = set()\n",
        "    for syn in wordnet.synsets(word, pos=pos):\n",
        "      for lemma in syn.lemmas():\n",
        "        for antonym in lemma.antonyms():\n",
        "          antonyms.add(antonym.name())\n",
        "    if len(antonyms) == 1:\n",
        "      return antonyms.pop()\n",
        "    else:\n",
        "      return None\n",
        "\n",
        "def replaceNegations(text):\n",
        "    \"\"\" Finds \"not\" and antonym for the next word and if found, replaces not and the next word with the antonym \"\"\"\n",
        "    i, l = 0, len(text)\n",
        "    words = []\n",
        "    while i < l:\n",
        "      word = text[i]\n",
        "      if word == 'not' and i+1 < l:\n",
        "        ant = replace(text[i+1])\n",
        "        if ant:\n",
        "          words.append(ant)\n",
        "          i += 2\n",
        "          continue\n",
        "      words.append(word)\n",
        "      i += 1\n",
        "    return words\n",
        "\n",
        "### Replace Negations End ###\n",
        "\n",
        "def addNotTag(text):\n",
        "\t\"\"\" Finds \"not,never,no\" and adds the tag NEG_ to all words that follow until the next punctuation \"\"\"\n",
        "\treturn text\n",
        "  #transformed = re.sub(r'\\b(?:not|never|no)\\b[\\w\\s]+[^\\w\\s]',\n",
        "  #     lambda match: re.sub(r'(\\s+)(\\w+)', r'\\1NEG_\\2', match.group(0)),\n",
        "  #     text,\n",
        "  #     flags=re.IGNORECASE)\n",
        "\t#return transformed\n",
        "  #return text\n",
        "\n",
        "def addCapTag(word):\n",
        "    \"\"\" Finds a word with at least 3 characters capitalized and adds the tag ALL_CAPS_ \"\"\"\n",
        "    return word\n",
        "    #if(len(re.findall(\"[A-Z]{3,}\", word))):\n",
        "    #    word = word.replace('\\\\', '' )\n",
        "    #    transformed = re.sub(\"[A-Z]{3,}\", \"ALL_CAPS_\"+word, word)\n",
        "    #   return transformed\n",
        "    #else:\n",
        "    #    return word\n",
        "    #return word\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hTme210DSVv1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nU2VrIGo6R9l"
      },
      "outputs": [],
      "source": [
        "\"\"\" Copyright 2017, Dimitrios Effrosynidis, All rights reserved. \"\"\"\n",
        "\n",
        "\n",
        "print(\"Starting preprocess..\\n\")\n",
        "\n",
        "\"\"\" Tokenizes a text to its words, removes and replaces some of them \"\"\"\n",
        "finalTokens = [] # all tokens\n",
        "stoplist = stopwords.words('english')\n",
        "my_stopwords = \"multiexclamation multiquestion multistop url atuser st rd nd th am pm of for\" # my extra stopwords\n",
        "stoplist = stoplist + my_stopwords.split()\n",
        "allowedWordTypes = [\"J\",\"R\",\"V\",\"N\"] #  J is Adject, R is Adverb, V is Verb, N is Noun. These are used for POS Tagging\n",
        "lemmatizer = WordNetLemmatizer() # set lemmatizer\n",
        "#stemmer = PorterStemmer() # set stemmer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q84H-JpCxqgD"
      },
      "source": [
        "# Loading the input file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QylbmY0kxo27"
      },
      "outputs": [],
      "source": [
        "csvDestinationFile = open(csvDestinationFileName, 'a+')\n",
        "csvWriter = csv.writer(csvDestinationFile)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fBdnbB2cA2HA"
      },
      "outputs": [],
      "source": [
        "csvDestinationFileName"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wjkH1dZnIqzn"
      },
      "outputs": [],
      "source": [
        "csvSourceFileName"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jleMu3Fo6R9-"
      },
      "outputs": [],
      "source": [
        "\"\"\" Copyright 2017, Dimitrios Effrosynidis, All rights reserved. \"\"\"\n",
        "print(\"Reading and loading dataset to memory\")\n",
        "#f = open(\"ss-twitterfinal.txt\",\"r\", encoding=\"utf8\", errors='replace').read()\n",
        "#df_original = pd.read_excel(DATASET_PATH, sheet_name=\"Stream2\")\n",
        "df_original = pd.read_csv(csvSourceFileName ,  header=None)\n",
        "df_proccessed = df_original\n",
        "\n",
        "print(\"The dataset is loaded\")\n",
        "print(\"the original dataset shape is\",df_original.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VUoFa316FkkY"
      },
      "outputs": [],
      "source": [
        "df_proccessed.head(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SW1ZttnDxzer"
      },
      "source": [
        "# Start to process"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sRCU3YDz6R-f"
      },
      "outputs": [],
      "source": [
        "print(\"The processing tweets start...\")\n",
        "t0 = time()\n",
        "totalSentences = 0\n",
        "totalEmoticons = 0\n",
        "totalSlangs = 0\n",
        "totalSlangsFound = []\n",
        "totalElongated = 0\n",
        "totalMultiExclamationMarks = 0\n",
        "totalMultiQuestionMarks = 0\n",
        "totalMultiStopMarks = 0\n",
        "totalAllCaps = 0\n",
        "\n",
        "i = 0\n",
        "for  row in df_proccessed.itertuples(index=False):\n",
        "    t1 = time()\n",
        "    strTemp = row[2]\n",
        "    onlyOneSentenceTokens = [] # tokens of one sentence each time\n",
        "\n",
        "    try:\n",
        "     strTemp = strTemp.encode('ascii', 'ignore').decode('unicode_escape')\n",
        "    except:\n",
        "     pass\n",
        "\n",
        "    strTemp = str(strTemp).lower()\n",
        "\n",
        "\n",
        "    strTemp = re.sub(r'(\\\\u[0-9A-Fa-f]+)',r'', strTemp)\n",
        "\n",
        "    strTemp = re.sub(r'[^\\x00-\\x7f]',r'',strTemp)\n",
        "\n",
        "    strTemp = replaceURL(strTemp) # Technique 1\n",
        "\n",
        "    strTemp = replaceAtUser(strTemp) # Technique 1\n",
        "\n",
        "    strTemp = removeHashtagInFrontOfWord(strTemp) # Technique 1\n",
        "\n",
        "    strTemp = removeNumbers(strTemp)\n",
        "\n",
        "    strTemp = removeEmoticons(strTemp)\n",
        "    #temp_slangs, temp_slangsFound = countSlang(strTemp)\n",
        "    #totalSlangs += temp_slangs # total slangs for all sentences\n",
        "    #for word in temp_slangsFound:\n",
        "    #    totalSlangsFound.append(word) # all the slangs found in all sentences\n",
        "\n",
        "    #strTemp = replaceSlang(strTemp) # Technique 2: replaces slang words and abbreviations with their equivalents\n",
        "    strTemp = replaceContraction(strTemp) # Technique 3: replaces contractions to their equivalents\n",
        "    strTemp = removeNumbers(strTemp) # Technique 4: remove integers from text\n",
        "\n",
        "    emoticons = countEmoticons(strTemp) # how many emoticons in this sentence\n",
        "    totalEmoticons += emoticons\n",
        "\n",
        "    strTemp = removeEmoticons(strTemp) # removes emoticons from text\n",
        "\n",
        "\n",
        "    #totalAllCaps += countAllCaps(strTemp)\n",
        "\n",
        "\n",
        "    strTemp = replaceMultiExclamationMark(strTemp) # Technique 5: replaces repetitions of exlamation marks with the tag \"multiExclamation\"\n",
        "    strTemp = replaceMultiQuestionMark(strTemp) # Technique 5: replaces repetitions of question marks with the tag \"multiQuestion\"\n",
        "    strTemp = replaceMultiStopMark(strTemp) # Technique 5: replaces repetitions of stop marks with the tag \"multiStop\"\n",
        "\n",
        "\n",
        "    translator = str.maketrans('', '', string.punctuation)\n",
        "    strTemp = strTemp.translate(translator) # Technique 7: remove punctuation\n",
        "\n",
        "    #totalElongated += countElongated(strTemp) # how many elongated words emoticons in this sentence\n",
        "\n",
        "\n",
        "    tokens = nltk.word_tokenize(strTemp) # it takes a text as an input and provides a list of every token in it\n",
        "    stopwords = stoplist\n",
        "    tokens = [ w for w in tokens if not w in stopwords ]\n",
        "### NO POS TAGGING BEGIN (If you don't want to use POS Tagging keep this section uncommented) ###\n",
        "\n",
        "    for w in tokens:\n",
        "\n",
        "        if (w not in stoplist): # Technique 10: remove stopwords\n",
        "            #final_word = addCapTag(w) # Technique 8: Finds a word with at least 3 characters capitalized and adds the tag ALL_CAPS_\n",
        "            final_word = w.lower() # Technique 9: lowercases all characters\n",
        "            final_word = replaceElongated(final_word) # Technique 11: replaces an elongated word with its basic form, unless the word exists in the lexicon\n",
        "            #if len(final_word)>1:\n",
        "            #    final_word = spellCorrection(final_word) # Technique 12: correction of spelling errors\n",
        "            #final_word = lemmatizer.lemmatize(final_word) # Technique 14: lemmatizes words\n",
        "            final_word = stemmer.stem(final_word) # Technique 15: apply stemming to words\n",
        "            onlyOneSentenceTokens.append(final_word)\n",
        "\n",
        "    #tokens = [ w.lower() for w in tokens]\n",
        "    #stopWords = set(stopwords.words('english'))\n",
        "\n",
        "\n",
        "\n",
        "    onlyOneSentenceTokens = \" \".join(onlyOneSentenceTokens)\n",
        "\n",
        "    csvWriter.writerow([ str(row[0]) , onlyOneSentenceTokens, str(row[1]) ])\n",
        "    #csvWriter.writerow([  onlyOneSentenceTokens, str(row[0]) ])\n",
        "    #i = i + 1\n",
        "\n",
        "\n",
        "\n",
        "print(\"The processing tweets end\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "36DdZ9786R-r"
      },
      "outputs": [],
      "source": [
        "csvDestinationFile.close()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}